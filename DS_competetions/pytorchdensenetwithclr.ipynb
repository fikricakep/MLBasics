{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorchdensenetwithclr.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanchit2843/ds_competitions/blob/master/pytorchdensenetwithclr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "YPl-is95_F2f",
        "colab_type": "code",
        "outputId": "9a01c00c-a7f5-4f71-b0b0-9a6b27facdd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "#print(os.listdir(\"/kaggle/input/test\"))\n",
        "!pip install efficientnet_pytorch\n",
        "!pip install torchsummary\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "import torchvision\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import os\n",
        "import torch\n",
        "from tqdm.autonotebook import tqdm\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/72/a3/68e95f068658fd26ef2757980e7a177faf7678d1fd8d7f7306a75250145c/efficientnet_pytorch-0.2.0.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.16.4)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/20/c7/6d529b628dd965b5cfdba09b3544408c6d3f34a479453d0fe3\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.2.0\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quOf8T-v_Ja5",
        "colab_type": "code",
        "outputId": "e86be9ae-7769-45ca-cf4d-c4c041793375",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "!pip install -i https://test.pypi.org/simple/ supportlib\n",
        "import supportlib.gettingdata as getdata\n",
        "getdata.kaggle()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://test.pypi.org/simple/\n",
            "Collecting supportlib\n",
            "  Downloading https://test-files.pythonhosted.org/packages/c7/e8/a44bb606fca2603f0c79e8593fe0f6f1626dee5bad5177afb9ee260fd223/supportlib-0.1.0-py3-none-any.whl\n",
            "Installing collected packages: supportlib\n",
            "Successfully installed supportlib-0.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-25e601c8-173a-495b-99ea-d1737d8a3143\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-25e601c8-173a-495b-99ea-d1737d8a3143\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fonNroHp_Yuj",
        "colab_type": "code",
        "outputId": "3352de86-ce1d-4ca1-e1e6-69d3cb6b3777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "!kaggle competitions download -c aerial-cactus-identification"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading train.csv to /content\n",
            "\r  0% 0.00/667k [00:00<?, ?B/s]\n",
            "100% 667k/667k [00:00<00:00, 43.1MB/s]\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/160k [00:00<?, ?B/s]\n",
            "100% 160k/160k [00:00<00:00, 52.7MB/s]\n",
            "Downloading test.zip to /content\n",
            "  0% 0.00/4.20M [00:00<?, ?B/s]\n",
            "100% 4.20M/4.20M [00:00<00:00, 38.7MB/s]\n",
            "Downloading train.zip to /content\n",
            " 47% 9.00M/19.2M [00:01<00:00, 11.1MB/s]\n",
            "100% 19.2M/19.2M [00:01<00:00, 16.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq6xCn3P_l4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "getdata.zipextract('/content/test.zip')\n",
        "getdata.zipextract('/content/train.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WBmJUFdf_F2j",
        "colab_type": "code",
        "outputId": "c388e8fb-cfff-4ab8-a4e1-25ec093645cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "train_csv = pd.read_csv('/content/train.csv')\n",
        "train_csv.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>has_cactus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0004be2cfeaba1c0361d39e2b000257b.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000c8a36845c0208e833c79c1bffedd1.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000d1e9a533f62e55c289303b072733d.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0011485b40695e9138e92d0b3fb55128.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0014d7a11e90b62848904c1418fc8cf2.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0017c3c18ddd57a2ea6f9848c79d83d2.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>002134abf28af54575c18741b89dd2a4.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0024320f43bdd490562246435af4f90b.jpg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>002930423b9840e67e5a54afd4768a1e.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00351838ebf6dff6e53056e00a1e307c.jpg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     id  has_cactus\n",
              "0  0004be2cfeaba1c0361d39e2b000257b.jpg           1\n",
              "1  000c8a36845c0208e833c79c1bffedd1.jpg           1\n",
              "2  000d1e9a533f62e55c289303b072733d.jpg           1\n",
              "3  0011485b40695e9138e92d0b3fb55128.jpg           1\n",
              "4  0014d7a11e90b62848904c1418fc8cf2.jpg           1\n",
              "5  0017c3c18ddd57a2ea6f9848c79d83d2.jpg           1\n",
              "6  002134abf28af54575c18741b89dd2a4.jpg           0\n",
              "7  0024320f43bdd490562246435af4f90b.jpg           0\n",
              "8  002930423b9840e67e5a54afd4768a1e.jpg           1\n",
              "9  00351838ebf6dff6e53056e00a1e307c.jpg           1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EMuCI-j2_F2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = train_csv['has_cactus'].unique()\n",
        "encoder = {0:'no cactus',1:'has cactus'}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qJSsNVaG_F2r",
        "colab_type": "code",
        "outputId": "d836edb6-5c26-40fe-bdbf-8bf3be0d07c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "'''from sklearn.model_selection import train_test_split\n",
        "train_df,val_df = train_test_split(train_csv,test_size = 0)\n",
        "val_df = val_df.reset_index()\n",
        "val_df = val_df.drop(['index'],axis = 1)\n",
        "train_df = train_df.reset_index()\n",
        "train_df = train_df.drop(['index'],axis = 1)'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"from sklearn.model_selection import train_test_split\\ntrain_df,val_df = train_test_split(train_csv,test_size = 0)\\nval_df = val_df.reset_index()\\nval_df = val_df.drop(['index'],axis = 1)\\ntrain_df = train_df.reset_index()\\ntrain_df = train_df.drop(['index'],axis = 1)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1LDdT3eN_F2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class cactus_dataset(Dataset):\n",
        "  def __init__(self,image_dir,train_csv,transform = None):\n",
        "    self.img_dir = image_dir\n",
        "    self.transform = transform\n",
        "    self.id = train_csv.iloc[:,0]\n",
        "    self.classes =  train_csv.iloc[:,1]\n",
        "  def __len__(self):\n",
        "    return len(self.id)\n",
        "  def __getitem__(self,idx):\n",
        "    img_name = os.path.join(self.img_dir, self.id[idx])\n",
        "    image = cv2.imread(img_name)\n",
        "    if self.transform:\n",
        "        image = self.transform(image)\n",
        "    label = self.classes[idx]\n",
        "    return image,label\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "d3a7CxNW_F2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "CfGVlCdX_F21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "train_transforms = transforms.Compose([\n",
        "                                        transforms.ToPILImage(),\n",
        "                                        transforms.Resize(128),\n",
        "                                        transforms.RandomHorizontalFlip(),\n",
        "                                        transforms.RandomVerticalFlip(),\n",
        "                                        transforms.RandomRotation(10),\n",
        "                                        transforms.RandomAffine(10, translate=None, scale=None, shear=2, resample=False, fillcolor=0),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                                                            [0.229, 0.224, 0.225])])\n",
        "test_transforms = transforms.Compose([\n",
        "                                        transforms.ToPILImage(),\n",
        "                                        transforms.Resize(128),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                                                            [0.229, 0.224, 0.225])])\n",
        "\n",
        "#inverse normalization for image plot\n",
        "train_data = cactus_dataset('/content/train',train_csv,transform = train_transforms)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "r8aH_T-r_F24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#val_data = cactus_dataset('/kaggle/input/train/train',val_df,transform = test_transforms)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n",
        "#val_loader = DataLoader(val_data, batch_size=4,shuffle=True, num_workers=0)\n",
        "dataloaders = {'train':train_loader}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0KcvNhcZ_F28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from efficientnet_pytorch import EfficientNet\n",
        "import torchvision\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import os\n",
        "import torch\n",
        "from tqdm.autonotebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class classifie(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(classifie, self).__init__()\n",
        "        model = models.densenet201(pretrained = True)\n",
        "        model = model.features\n",
        "        for child in model.children():\n",
        "          for layer in child.modules():\n",
        "            layer.requires_grad = False\n",
        "            if(isinstance(layer,torch.nn.modules.batchnorm.BatchNorm2d)):\n",
        "              layer.requires_grad = True\n",
        "        #model = EfficientNet.from_pretrained('efficientnet-b3')\n",
        "        #model =  nn.Sequential(*list(model.children())[:-3])\n",
        "        self.model = model\n",
        "        self.linear = nn.Linear(3840, 512)\n",
        "        self.bn = nn.BatchNorm1d(512)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.elu = nn.ELU()\n",
        "        self.out = nn.Linear(512, 2)\n",
        "        self.bn1 = nn.BatchNorm1d(3840)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "        avg_pool = nn.functional.adaptive_avg_pool2d(out, output_size = 1)\n",
        "        max_pool = nn.functional.adaptive_max_pool2d(out, output_size = 1)\n",
        "        out = torch.cat((avg_pool,max_pool),1)\n",
        "        batch = out.shape[0]\n",
        "        out = out.view(batch, -1)\n",
        "        conc = self.linear(self.dropout2(self.bn1(out)))\n",
        "        conc = self.elu(conc)\n",
        "        conc = self.bn(conc)\n",
        "        conc = self.dropout(conc)\n",
        "        res = self.out(conc)\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "q2s9VwCs_F3D",
        "colab_type": "code",
        "outputId": "ce4590e6-2352-408a-9e24-edb446367ca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11017
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "classifier1 = classifie().to(device)\n",
        "from torchsummary import summary\n",
        "summary(classifier1,(3,32,32))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
            "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
            "              ReLU-3           [-1, 64, 16, 16]               0\n",
            "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
            "       BatchNorm2d-5             [-1, 64, 8, 8]             128\n",
            "              ReLU-6             [-1, 64, 8, 8]               0\n",
            "            Conv2d-7            [-1, 128, 8, 8]           8,192\n",
            "       BatchNorm2d-8            [-1, 128, 8, 8]             256\n",
            "              ReLU-9            [-1, 128, 8, 8]               0\n",
            "           Conv2d-10             [-1, 32, 8, 8]          36,864\n",
            "      BatchNorm2d-11             [-1, 96, 8, 8]             192\n",
            "             ReLU-12             [-1, 96, 8, 8]               0\n",
            "           Conv2d-13            [-1, 128, 8, 8]          12,288\n",
            "      BatchNorm2d-14            [-1, 128, 8, 8]             256\n",
            "             ReLU-15            [-1, 128, 8, 8]               0\n",
            "           Conv2d-16             [-1, 32, 8, 8]          36,864\n",
            "      BatchNorm2d-17            [-1, 128, 8, 8]             256\n",
            "             ReLU-18            [-1, 128, 8, 8]               0\n",
            "           Conv2d-19            [-1, 128, 8, 8]          16,384\n",
            "      BatchNorm2d-20            [-1, 128, 8, 8]             256\n",
            "             ReLU-21            [-1, 128, 8, 8]               0\n",
            "           Conv2d-22             [-1, 32, 8, 8]          36,864\n",
            "      BatchNorm2d-23            [-1, 160, 8, 8]             320\n",
            "             ReLU-24            [-1, 160, 8, 8]               0\n",
            "           Conv2d-25            [-1, 128, 8, 8]          20,480\n",
            "      BatchNorm2d-26            [-1, 128, 8, 8]             256\n",
            "             ReLU-27            [-1, 128, 8, 8]               0\n",
            "           Conv2d-28             [-1, 32, 8, 8]          36,864\n",
            "      BatchNorm2d-29            [-1, 192, 8, 8]             384\n",
            "             ReLU-30            [-1, 192, 8, 8]               0\n",
            "           Conv2d-31            [-1, 128, 8, 8]          24,576\n",
            "      BatchNorm2d-32            [-1, 128, 8, 8]             256\n",
            "             ReLU-33            [-1, 128, 8, 8]               0\n",
            "           Conv2d-34             [-1, 32, 8, 8]          36,864\n",
            "      BatchNorm2d-35            [-1, 224, 8, 8]             448\n",
            "             ReLU-36            [-1, 224, 8, 8]               0\n",
            "           Conv2d-37            [-1, 128, 8, 8]          28,672\n",
            "      BatchNorm2d-38            [-1, 128, 8, 8]             256\n",
            "             ReLU-39            [-1, 128, 8, 8]               0\n",
            "           Conv2d-40             [-1, 32, 8, 8]          36,864\n",
            "      BatchNorm2d-41            [-1, 256, 8, 8]             512\n",
            "             ReLU-42            [-1, 256, 8, 8]               0\n",
            "           Conv2d-43            [-1, 128, 8, 8]          32,768\n",
            "        AvgPool2d-44            [-1, 128, 4, 4]               0\n",
            "      BatchNorm2d-45            [-1, 128, 4, 4]             256\n",
            "             ReLU-46            [-1, 128, 4, 4]               0\n",
            "           Conv2d-47            [-1, 128, 4, 4]          16,384\n",
            "      BatchNorm2d-48            [-1, 128, 4, 4]             256\n",
            "             ReLU-49            [-1, 128, 4, 4]               0\n",
            "           Conv2d-50             [-1, 32, 4, 4]          36,864\n",
            "      BatchNorm2d-51            [-1, 160, 4, 4]             320\n",
            "             ReLU-52            [-1, 160, 4, 4]               0\n",
            "           Conv2d-53            [-1, 128, 4, 4]          20,480\n",
            "      BatchNorm2d-54            [-1, 128, 4, 4]             256\n",
            "             ReLU-55            [-1, 128, 4, 4]               0\n",
            "           Conv2d-56             [-1, 32, 4, 4]          36,864\n",
            "      BatchNorm2d-57            [-1, 192, 4, 4]             384\n",
            "             ReLU-58            [-1, 192, 4, 4]               0\n",
            "           Conv2d-59            [-1, 128, 4, 4]          24,576\n",
            "      BatchNorm2d-60            [-1, 128, 4, 4]             256\n",
            "             ReLU-61            [-1, 128, 4, 4]               0\n",
            "           Conv2d-62             [-1, 32, 4, 4]          36,864\n",
            "      BatchNorm2d-63            [-1, 224, 4, 4]             448\n",
            "             ReLU-64            [-1, 224, 4, 4]               0\n",
            "           Conv2d-65            [-1, 128, 4, 4]          28,672\n",
            "      BatchNorm2d-66            [-1, 128, 4, 4]             256\n",
            "             ReLU-67            [-1, 128, 4, 4]               0\n",
            "           Conv2d-68             [-1, 32, 4, 4]          36,864\n",
            "      BatchNorm2d-69            [-1, 256, 4, 4]             512\n",
            "             ReLU-70            [-1, 256, 4, 4]               0\n",
            "           Conv2d-71            [-1, 128, 4, 4]          32,768\n",
            "      BatchNorm2d-72            [-1, 128, 4, 4]             256\n",
            "             ReLU-73            [-1, 128, 4, 4]               0\n",
            "           Conv2d-74             [-1, 32, 4, 4]          36,864\n",
            "      BatchNorm2d-75            [-1, 288, 4, 4]             576\n",
            "             ReLU-76            [-1, 288, 4, 4]               0\n",
            "           Conv2d-77            [-1, 128, 4, 4]          36,864\n",
            "      BatchNorm2d-78            [-1, 128, 4, 4]             256\n",
            "             ReLU-79            [-1, 128, 4, 4]               0\n",
            "           Conv2d-80             [-1, 32, 4, 4]          36,864\n",
            "      BatchNorm2d-81            [-1, 320, 4, 4]             640\n",
            "             ReLU-82            [-1, 320, 4, 4]               0\n",
            "           Conv2d-83            [-1, 128, 4, 4]          40,960\n",
            "      BatchNorm2d-84            [-1, 128, 4, 4]             256\n",
            "             ReLU-85            [-1, 128, 4, 4]               0\n",
            "           Conv2d-86             [-1, 32, 4, 4]          36,864\n",
            "      BatchNorm2d-87            [-1, 352, 4, 4]             704\n",
            "             ReLU-88            [-1, 352, 4, 4]               0\n",
            "           Conv2d-89            [-1, 128, 4, 4]          45,056\n",
            "      BatchNorm2d-90            [-1, 128, 4, 4]             256\n",
            "             ReLU-91            [-1, 128, 4, 4]               0\n",
            "           Conv2d-92             [-1, 32, 4, 4]          36,864\n",
            "      BatchNorm2d-93            [-1, 384, 4, 4]             768\n",
            "             ReLU-94            [-1, 384, 4, 4]               0\n",
            "           Conv2d-95            [-1, 128, 4, 4]          49,152\n",
            "      BatchNorm2d-96            [-1, 128, 4, 4]             256\n",
            "             ReLU-97            [-1, 128, 4, 4]               0\n",
            "           Conv2d-98             [-1, 32, 4, 4]          36,864\n",
            "      BatchNorm2d-99            [-1, 416, 4, 4]             832\n",
            "            ReLU-100            [-1, 416, 4, 4]               0\n",
            "          Conv2d-101            [-1, 128, 4, 4]          53,248\n",
            "     BatchNorm2d-102            [-1, 128, 4, 4]             256\n",
            "            ReLU-103            [-1, 128, 4, 4]               0\n",
            "          Conv2d-104             [-1, 32, 4, 4]          36,864\n",
            "     BatchNorm2d-105            [-1, 448, 4, 4]             896\n",
            "            ReLU-106            [-1, 448, 4, 4]               0\n",
            "          Conv2d-107            [-1, 128, 4, 4]          57,344\n",
            "     BatchNorm2d-108            [-1, 128, 4, 4]             256\n",
            "            ReLU-109            [-1, 128, 4, 4]               0\n",
            "          Conv2d-110             [-1, 32, 4, 4]          36,864\n",
            "     BatchNorm2d-111            [-1, 480, 4, 4]             960\n",
            "            ReLU-112            [-1, 480, 4, 4]               0\n",
            "          Conv2d-113            [-1, 128, 4, 4]          61,440\n",
            "     BatchNorm2d-114            [-1, 128, 4, 4]             256\n",
            "            ReLU-115            [-1, 128, 4, 4]               0\n",
            "          Conv2d-116             [-1, 32, 4, 4]          36,864\n",
            "     BatchNorm2d-117            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-118            [-1, 512, 4, 4]               0\n",
            "          Conv2d-119            [-1, 256, 4, 4]         131,072\n",
            "       AvgPool2d-120            [-1, 256, 2, 2]               0\n",
            "     BatchNorm2d-121            [-1, 256, 2, 2]             512\n",
            "            ReLU-122            [-1, 256, 2, 2]               0\n",
            "          Conv2d-123            [-1, 128, 2, 2]          32,768\n",
            "     BatchNorm2d-124            [-1, 128, 2, 2]             256\n",
            "            ReLU-125            [-1, 128, 2, 2]               0\n",
            "          Conv2d-126             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-127            [-1, 288, 2, 2]             576\n",
            "            ReLU-128            [-1, 288, 2, 2]               0\n",
            "          Conv2d-129            [-1, 128, 2, 2]          36,864\n",
            "     BatchNorm2d-130            [-1, 128, 2, 2]             256\n",
            "            ReLU-131            [-1, 128, 2, 2]               0\n",
            "          Conv2d-132             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-133            [-1, 320, 2, 2]             640\n",
            "            ReLU-134            [-1, 320, 2, 2]               0\n",
            "          Conv2d-135            [-1, 128, 2, 2]          40,960\n",
            "     BatchNorm2d-136            [-1, 128, 2, 2]             256\n",
            "            ReLU-137            [-1, 128, 2, 2]               0\n",
            "          Conv2d-138             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-139            [-1, 352, 2, 2]             704\n",
            "            ReLU-140            [-1, 352, 2, 2]               0\n",
            "          Conv2d-141            [-1, 128, 2, 2]          45,056\n",
            "     BatchNorm2d-142            [-1, 128, 2, 2]             256\n",
            "            ReLU-143            [-1, 128, 2, 2]               0\n",
            "          Conv2d-144             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-145            [-1, 384, 2, 2]             768\n",
            "            ReLU-146            [-1, 384, 2, 2]               0\n",
            "          Conv2d-147            [-1, 128, 2, 2]          49,152\n",
            "     BatchNorm2d-148            [-1, 128, 2, 2]             256\n",
            "            ReLU-149            [-1, 128, 2, 2]               0\n",
            "          Conv2d-150             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-151            [-1, 416, 2, 2]             832\n",
            "            ReLU-152            [-1, 416, 2, 2]               0\n",
            "          Conv2d-153            [-1, 128, 2, 2]          53,248\n",
            "     BatchNorm2d-154            [-1, 128, 2, 2]             256\n",
            "            ReLU-155            [-1, 128, 2, 2]               0\n",
            "          Conv2d-156             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-157            [-1, 448, 2, 2]             896\n",
            "            ReLU-158            [-1, 448, 2, 2]               0\n",
            "          Conv2d-159            [-1, 128, 2, 2]          57,344\n",
            "     BatchNorm2d-160            [-1, 128, 2, 2]             256\n",
            "            ReLU-161            [-1, 128, 2, 2]               0\n",
            "          Conv2d-162             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-163            [-1, 480, 2, 2]             960\n",
            "            ReLU-164            [-1, 480, 2, 2]               0\n",
            "          Conv2d-165            [-1, 128, 2, 2]          61,440\n",
            "     BatchNorm2d-166            [-1, 128, 2, 2]             256\n",
            "            ReLU-167            [-1, 128, 2, 2]               0\n",
            "          Conv2d-168             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-169            [-1, 512, 2, 2]           1,024\n",
            "            ReLU-170            [-1, 512, 2, 2]               0\n",
            "          Conv2d-171            [-1, 128, 2, 2]          65,536\n",
            "     BatchNorm2d-172            [-1, 128, 2, 2]             256\n",
            "            ReLU-173            [-1, 128, 2, 2]               0\n",
            "          Conv2d-174             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-175            [-1, 544, 2, 2]           1,088\n",
            "            ReLU-176            [-1, 544, 2, 2]               0\n",
            "          Conv2d-177            [-1, 128, 2, 2]          69,632\n",
            "     BatchNorm2d-178            [-1, 128, 2, 2]             256\n",
            "            ReLU-179            [-1, 128, 2, 2]               0\n",
            "          Conv2d-180             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-181            [-1, 576, 2, 2]           1,152\n",
            "            ReLU-182            [-1, 576, 2, 2]               0\n",
            "          Conv2d-183            [-1, 128, 2, 2]          73,728\n",
            "     BatchNorm2d-184            [-1, 128, 2, 2]             256\n",
            "            ReLU-185            [-1, 128, 2, 2]               0\n",
            "          Conv2d-186             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-187            [-1, 608, 2, 2]           1,216\n",
            "            ReLU-188            [-1, 608, 2, 2]               0\n",
            "          Conv2d-189            [-1, 128, 2, 2]          77,824\n",
            "     BatchNorm2d-190            [-1, 128, 2, 2]             256\n",
            "            ReLU-191            [-1, 128, 2, 2]               0\n",
            "          Conv2d-192             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-193            [-1, 640, 2, 2]           1,280\n",
            "            ReLU-194            [-1, 640, 2, 2]               0\n",
            "          Conv2d-195            [-1, 128, 2, 2]          81,920\n",
            "     BatchNorm2d-196            [-1, 128, 2, 2]             256\n",
            "            ReLU-197            [-1, 128, 2, 2]               0\n",
            "          Conv2d-198             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-199            [-1, 672, 2, 2]           1,344\n",
            "            ReLU-200            [-1, 672, 2, 2]               0\n",
            "          Conv2d-201            [-1, 128, 2, 2]          86,016\n",
            "     BatchNorm2d-202            [-1, 128, 2, 2]             256\n",
            "            ReLU-203            [-1, 128, 2, 2]               0\n",
            "          Conv2d-204             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-205            [-1, 704, 2, 2]           1,408\n",
            "            ReLU-206            [-1, 704, 2, 2]               0\n",
            "          Conv2d-207            [-1, 128, 2, 2]          90,112\n",
            "     BatchNorm2d-208            [-1, 128, 2, 2]             256\n",
            "            ReLU-209            [-1, 128, 2, 2]               0\n",
            "          Conv2d-210             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-211            [-1, 736, 2, 2]           1,472\n",
            "            ReLU-212            [-1, 736, 2, 2]               0\n",
            "          Conv2d-213            [-1, 128, 2, 2]          94,208\n",
            "     BatchNorm2d-214            [-1, 128, 2, 2]             256\n",
            "            ReLU-215            [-1, 128, 2, 2]               0\n",
            "          Conv2d-216             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-217            [-1, 768, 2, 2]           1,536\n",
            "            ReLU-218            [-1, 768, 2, 2]               0\n",
            "          Conv2d-219            [-1, 128, 2, 2]          98,304\n",
            "     BatchNorm2d-220            [-1, 128, 2, 2]             256\n",
            "            ReLU-221            [-1, 128, 2, 2]               0\n",
            "          Conv2d-222             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-223            [-1, 800, 2, 2]           1,600\n",
            "            ReLU-224            [-1, 800, 2, 2]               0\n",
            "          Conv2d-225            [-1, 128, 2, 2]         102,400\n",
            "     BatchNorm2d-226            [-1, 128, 2, 2]             256\n",
            "            ReLU-227            [-1, 128, 2, 2]               0\n",
            "          Conv2d-228             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-229            [-1, 832, 2, 2]           1,664\n",
            "            ReLU-230            [-1, 832, 2, 2]               0\n",
            "          Conv2d-231            [-1, 128, 2, 2]         106,496\n",
            "     BatchNorm2d-232            [-1, 128, 2, 2]             256\n",
            "            ReLU-233            [-1, 128, 2, 2]               0\n",
            "          Conv2d-234             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-235            [-1, 864, 2, 2]           1,728\n",
            "            ReLU-236            [-1, 864, 2, 2]               0\n",
            "          Conv2d-237            [-1, 128, 2, 2]         110,592\n",
            "     BatchNorm2d-238            [-1, 128, 2, 2]             256\n",
            "            ReLU-239            [-1, 128, 2, 2]               0\n",
            "          Conv2d-240             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-241            [-1, 896, 2, 2]           1,792\n",
            "            ReLU-242            [-1, 896, 2, 2]               0\n",
            "          Conv2d-243            [-1, 128, 2, 2]         114,688\n",
            "     BatchNorm2d-244            [-1, 128, 2, 2]             256\n",
            "            ReLU-245            [-1, 128, 2, 2]               0\n",
            "          Conv2d-246             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-247            [-1, 928, 2, 2]           1,856\n",
            "            ReLU-248            [-1, 928, 2, 2]               0\n",
            "          Conv2d-249            [-1, 128, 2, 2]         118,784\n",
            "     BatchNorm2d-250            [-1, 128, 2, 2]             256\n",
            "            ReLU-251            [-1, 128, 2, 2]               0\n",
            "          Conv2d-252             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-253            [-1, 960, 2, 2]           1,920\n",
            "            ReLU-254            [-1, 960, 2, 2]               0\n",
            "          Conv2d-255            [-1, 128, 2, 2]         122,880\n",
            "     BatchNorm2d-256            [-1, 128, 2, 2]             256\n",
            "            ReLU-257            [-1, 128, 2, 2]               0\n",
            "          Conv2d-258             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-259            [-1, 992, 2, 2]           1,984\n",
            "            ReLU-260            [-1, 992, 2, 2]               0\n",
            "          Conv2d-261            [-1, 128, 2, 2]         126,976\n",
            "     BatchNorm2d-262            [-1, 128, 2, 2]             256\n",
            "            ReLU-263            [-1, 128, 2, 2]               0\n",
            "          Conv2d-264             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-265           [-1, 1024, 2, 2]           2,048\n",
            "            ReLU-266           [-1, 1024, 2, 2]               0\n",
            "          Conv2d-267            [-1, 128, 2, 2]         131,072\n",
            "     BatchNorm2d-268            [-1, 128, 2, 2]             256\n",
            "            ReLU-269            [-1, 128, 2, 2]               0\n",
            "          Conv2d-270             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-271           [-1, 1056, 2, 2]           2,112\n",
            "            ReLU-272           [-1, 1056, 2, 2]               0\n",
            "          Conv2d-273            [-1, 128, 2, 2]         135,168\n",
            "     BatchNorm2d-274            [-1, 128, 2, 2]             256\n",
            "            ReLU-275            [-1, 128, 2, 2]               0\n",
            "          Conv2d-276             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-277           [-1, 1088, 2, 2]           2,176\n",
            "            ReLU-278           [-1, 1088, 2, 2]               0\n",
            "          Conv2d-279            [-1, 128, 2, 2]         139,264\n",
            "     BatchNorm2d-280            [-1, 128, 2, 2]             256\n",
            "            ReLU-281            [-1, 128, 2, 2]               0\n",
            "          Conv2d-282             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-283           [-1, 1120, 2, 2]           2,240\n",
            "            ReLU-284           [-1, 1120, 2, 2]               0\n",
            "          Conv2d-285            [-1, 128, 2, 2]         143,360\n",
            "     BatchNorm2d-286            [-1, 128, 2, 2]             256\n",
            "            ReLU-287            [-1, 128, 2, 2]               0\n",
            "          Conv2d-288             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-289           [-1, 1152, 2, 2]           2,304\n",
            "            ReLU-290           [-1, 1152, 2, 2]               0\n",
            "          Conv2d-291            [-1, 128, 2, 2]         147,456\n",
            "     BatchNorm2d-292            [-1, 128, 2, 2]             256\n",
            "            ReLU-293            [-1, 128, 2, 2]               0\n",
            "          Conv2d-294             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-295           [-1, 1184, 2, 2]           2,368\n",
            "            ReLU-296           [-1, 1184, 2, 2]               0\n",
            "          Conv2d-297            [-1, 128, 2, 2]         151,552\n",
            "     BatchNorm2d-298            [-1, 128, 2, 2]             256\n",
            "            ReLU-299            [-1, 128, 2, 2]               0\n",
            "          Conv2d-300             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-301           [-1, 1216, 2, 2]           2,432\n",
            "            ReLU-302           [-1, 1216, 2, 2]               0\n",
            "          Conv2d-303            [-1, 128, 2, 2]         155,648\n",
            "     BatchNorm2d-304            [-1, 128, 2, 2]             256\n",
            "            ReLU-305            [-1, 128, 2, 2]               0\n",
            "          Conv2d-306             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-307           [-1, 1248, 2, 2]           2,496\n",
            "            ReLU-308           [-1, 1248, 2, 2]               0\n",
            "          Conv2d-309            [-1, 128, 2, 2]         159,744\n",
            "     BatchNorm2d-310            [-1, 128, 2, 2]             256\n",
            "            ReLU-311            [-1, 128, 2, 2]               0\n",
            "          Conv2d-312             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-313           [-1, 1280, 2, 2]           2,560\n",
            "            ReLU-314           [-1, 1280, 2, 2]               0\n",
            "          Conv2d-315            [-1, 128, 2, 2]         163,840\n",
            "     BatchNorm2d-316            [-1, 128, 2, 2]             256\n",
            "            ReLU-317            [-1, 128, 2, 2]               0\n",
            "          Conv2d-318             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-319           [-1, 1312, 2, 2]           2,624\n",
            "            ReLU-320           [-1, 1312, 2, 2]               0\n",
            "          Conv2d-321            [-1, 128, 2, 2]         167,936\n",
            "     BatchNorm2d-322            [-1, 128, 2, 2]             256\n",
            "            ReLU-323            [-1, 128, 2, 2]               0\n",
            "          Conv2d-324             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-325           [-1, 1344, 2, 2]           2,688\n",
            "            ReLU-326           [-1, 1344, 2, 2]               0\n",
            "          Conv2d-327            [-1, 128, 2, 2]         172,032\n",
            "     BatchNorm2d-328            [-1, 128, 2, 2]             256\n",
            "            ReLU-329            [-1, 128, 2, 2]               0\n",
            "          Conv2d-330             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-331           [-1, 1376, 2, 2]           2,752\n",
            "            ReLU-332           [-1, 1376, 2, 2]               0\n",
            "          Conv2d-333            [-1, 128, 2, 2]         176,128\n",
            "     BatchNorm2d-334            [-1, 128, 2, 2]             256\n",
            "            ReLU-335            [-1, 128, 2, 2]               0\n",
            "          Conv2d-336             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-337           [-1, 1408, 2, 2]           2,816\n",
            "            ReLU-338           [-1, 1408, 2, 2]               0\n",
            "          Conv2d-339            [-1, 128, 2, 2]         180,224\n",
            "     BatchNorm2d-340            [-1, 128, 2, 2]             256\n",
            "            ReLU-341            [-1, 128, 2, 2]               0\n",
            "          Conv2d-342             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-343           [-1, 1440, 2, 2]           2,880\n",
            "            ReLU-344           [-1, 1440, 2, 2]               0\n",
            "          Conv2d-345            [-1, 128, 2, 2]         184,320\n",
            "     BatchNorm2d-346            [-1, 128, 2, 2]             256\n",
            "            ReLU-347            [-1, 128, 2, 2]               0\n",
            "          Conv2d-348             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-349           [-1, 1472, 2, 2]           2,944\n",
            "            ReLU-350           [-1, 1472, 2, 2]               0\n",
            "          Conv2d-351            [-1, 128, 2, 2]         188,416\n",
            "     BatchNorm2d-352            [-1, 128, 2, 2]             256\n",
            "            ReLU-353            [-1, 128, 2, 2]               0\n",
            "          Conv2d-354             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-355           [-1, 1504, 2, 2]           3,008\n",
            "            ReLU-356           [-1, 1504, 2, 2]               0\n",
            "          Conv2d-357            [-1, 128, 2, 2]         192,512\n",
            "     BatchNorm2d-358            [-1, 128, 2, 2]             256\n",
            "            ReLU-359            [-1, 128, 2, 2]               0\n",
            "          Conv2d-360             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-361           [-1, 1536, 2, 2]           3,072\n",
            "            ReLU-362           [-1, 1536, 2, 2]               0\n",
            "          Conv2d-363            [-1, 128, 2, 2]         196,608\n",
            "     BatchNorm2d-364            [-1, 128, 2, 2]             256\n",
            "            ReLU-365            [-1, 128, 2, 2]               0\n",
            "          Conv2d-366             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-367           [-1, 1568, 2, 2]           3,136\n",
            "            ReLU-368           [-1, 1568, 2, 2]               0\n",
            "          Conv2d-369            [-1, 128, 2, 2]         200,704\n",
            "     BatchNorm2d-370            [-1, 128, 2, 2]             256\n",
            "            ReLU-371            [-1, 128, 2, 2]               0\n",
            "          Conv2d-372             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-373           [-1, 1600, 2, 2]           3,200\n",
            "            ReLU-374           [-1, 1600, 2, 2]               0\n",
            "          Conv2d-375            [-1, 128, 2, 2]         204,800\n",
            "     BatchNorm2d-376            [-1, 128, 2, 2]             256\n",
            "            ReLU-377            [-1, 128, 2, 2]               0\n",
            "          Conv2d-378             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-379           [-1, 1632, 2, 2]           3,264\n",
            "            ReLU-380           [-1, 1632, 2, 2]               0\n",
            "          Conv2d-381            [-1, 128, 2, 2]         208,896\n",
            "     BatchNorm2d-382            [-1, 128, 2, 2]             256\n",
            "            ReLU-383            [-1, 128, 2, 2]               0\n",
            "          Conv2d-384             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-385           [-1, 1664, 2, 2]           3,328\n",
            "            ReLU-386           [-1, 1664, 2, 2]               0\n",
            "          Conv2d-387            [-1, 128, 2, 2]         212,992\n",
            "     BatchNorm2d-388            [-1, 128, 2, 2]             256\n",
            "            ReLU-389            [-1, 128, 2, 2]               0\n",
            "          Conv2d-390             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-391           [-1, 1696, 2, 2]           3,392\n",
            "            ReLU-392           [-1, 1696, 2, 2]               0\n",
            "          Conv2d-393            [-1, 128, 2, 2]         217,088\n",
            "     BatchNorm2d-394            [-1, 128, 2, 2]             256\n",
            "            ReLU-395            [-1, 128, 2, 2]               0\n",
            "          Conv2d-396             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-397           [-1, 1728, 2, 2]           3,456\n",
            "            ReLU-398           [-1, 1728, 2, 2]               0\n",
            "          Conv2d-399            [-1, 128, 2, 2]         221,184\n",
            "     BatchNorm2d-400            [-1, 128, 2, 2]             256\n",
            "            ReLU-401            [-1, 128, 2, 2]               0\n",
            "          Conv2d-402             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-403           [-1, 1760, 2, 2]           3,520\n",
            "            ReLU-404           [-1, 1760, 2, 2]               0\n",
            "          Conv2d-405            [-1, 128, 2, 2]         225,280\n",
            "     BatchNorm2d-406            [-1, 128, 2, 2]             256\n",
            "            ReLU-407            [-1, 128, 2, 2]               0\n",
            "          Conv2d-408             [-1, 32, 2, 2]          36,864\n",
            "     BatchNorm2d-409           [-1, 1792, 2, 2]           3,584\n",
            "            ReLU-410           [-1, 1792, 2, 2]               0\n",
            "          Conv2d-411            [-1, 896, 2, 2]       1,605,632\n",
            "       AvgPool2d-412            [-1, 896, 1, 1]               0\n",
            "     BatchNorm2d-413            [-1, 896, 1, 1]           1,792\n",
            "            ReLU-414            [-1, 896, 1, 1]               0\n",
            "          Conv2d-415            [-1, 128, 1, 1]         114,688\n",
            "     BatchNorm2d-416            [-1, 128, 1, 1]             256\n",
            "            ReLU-417            [-1, 128, 1, 1]               0\n",
            "          Conv2d-418             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-419            [-1, 928, 1, 1]           1,856\n",
            "            ReLU-420            [-1, 928, 1, 1]               0\n",
            "          Conv2d-421            [-1, 128, 1, 1]         118,784\n",
            "     BatchNorm2d-422            [-1, 128, 1, 1]             256\n",
            "            ReLU-423            [-1, 128, 1, 1]               0\n",
            "          Conv2d-424             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-425            [-1, 960, 1, 1]           1,920\n",
            "            ReLU-426            [-1, 960, 1, 1]               0\n",
            "          Conv2d-427            [-1, 128, 1, 1]         122,880\n",
            "     BatchNorm2d-428            [-1, 128, 1, 1]             256\n",
            "            ReLU-429            [-1, 128, 1, 1]               0\n",
            "          Conv2d-430             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-431            [-1, 992, 1, 1]           1,984\n",
            "            ReLU-432            [-1, 992, 1, 1]               0\n",
            "          Conv2d-433            [-1, 128, 1, 1]         126,976\n",
            "     BatchNorm2d-434            [-1, 128, 1, 1]             256\n",
            "            ReLU-435            [-1, 128, 1, 1]               0\n",
            "          Conv2d-436             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-437           [-1, 1024, 1, 1]           2,048\n",
            "            ReLU-438           [-1, 1024, 1, 1]               0\n",
            "          Conv2d-439            [-1, 128, 1, 1]         131,072\n",
            "     BatchNorm2d-440            [-1, 128, 1, 1]             256\n",
            "            ReLU-441            [-1, 128, 1, 1]               0\n",
            "          Conv2d-442             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-443           [-1, 1056, 1, 1]           2,112\n",
            "            ReLU-444           [-1, 1056, 1, 1]               0\n",
            "          Conv2d-445            [-1, 128, 1, 1]         135,168\n",
            "     BatchNorm2d-446            [-1, 128, 1, 1]             256\n",
            "            ReLU-447            [-1, 128, 1, 1]               0\n",
            "          Conv2d-448             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-449           [-1, 1088, 1, 1]           2,176\n",
            "            ReLU-450           [-1, 1088, 1, 1]               0\n",
            "          Conv2d-451            [-1, 128, 1, 1]         139,264\n",
            "     BatchNorm2d-452            [-1, 128, 1, 1]             256\n",
            "            ReLU-453            [-1, 128, 1, 1]               0\n",
            "          Conv2d-454             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-455           [-1, 1120, 1, 1]           2,240\n",
            "            ReLU-456           [-1, 1120, 1, 1]               0\n",
            "          Conv2d-457            [-1, 128, 1, 1]         143,360\n",
            "     BatchNorm2d-458            [-1, 128, 1, 1]             256\n",
            "            ReLU-459            [-1, 128, 1, 1]               0\n",
            "          Conv2d-460             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-461           [-1, 1152, 1, 1]           2,304\n",
            "            ReLU-462           [-1, 1152, 1, 1]               0\n",
            "          Conv2d-463            [-1, 128, 1, 1]         147,456\n",
            "     BatchNorm2d-464            [-1, 128, 1, 1]             256\n",
            "            ReLU-465            [-1, 128, 1, 1]               0\n",
            "          Conv2d-466             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-467           [-1, 1184, 1, 1]           2,368\n",
            "            ReLU-468           [-1, 1184, 1, 1]               0\n",
            "          Conv2d-469            [-1, 128, 1, 1]         151,552\n",
            "     BatchNorm2d-470            [-1, 128, 1, 1]             256\n",
            "            ReLU-471            [-1, 128, 1, 1]               0\n",
            "          Conv2d-472             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-473           [-1, 1216, 1, 1]           2,432\n",
            "            ReLU-474           [-1, 1216, 1, 1]               0\n",
            "          Conv2d-475            [-1, 128, 1, 1]         155,648\n",
            "     BatchNorm2d-476            [-1, 128, 1, 1]             256\n",
            "            ReLU-477            [-1, 128, 1, 1]               0\n",
            "          Conv2d-478             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-479           [-1, 1248, 1, 1]           2,496\n",
            "            ReLU-480           [-1, 1248, 1, 1]               0\n",
            "          Conv2d-481            [-1, 128, 1, 1]         159,744\n",
            "     BatchNorm2d-482            [-1, 128, 1, 1]             256\n",
            "            ReLU-483            [-1, 128, 1, 1]               0\n",
            "          Conv2d-484             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-485           [-1, 1280, 1, 1]           2,560\n",
            "            ReLU-486           [-1, 1280, 1, 1]               0\n",
            "          Conv2d-487            [-1, 128, 1, 1]         163,840\n",
            "     BatchNorm2d-488            [-1, 128, 1, 1]             256\n",
            "            ReLU-489            [-1, 128, 1, 1]               0\n",
            "          Conv2d-490             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-491           [-1, 1312, 1, 1]           2,624\n",
            "            ReLU-492           [-1, 1312, 1, 1]               0\n",
            "          Conv2d-493            [-1, 128, 1, 1]         167,936\n",
            "     BatchNorm2d-494            [-1, 128, 1, 1]             256\n",
            "            ReLU-495            [-1, 128, 1, 1]               0\n",
            "          Conv2d-496             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-497           [-1, 1344, 1, 1]           2,688\n",
            "            ReLU-498           [-1, 1344, 1, 1]               0\n",
            "          Conv2d-499            [-1, 128, 1, 1]         172,032\n",
            "     BatchNorm2d-500            [-1, 128, 1, 1]             256\n",
            "            ReLU-501            [-1, 128, 1, 1]               0\n",
            "          Conv2d-502             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-503           [-1, 1376, 1, 1]           2,752\n",
            "            ReLU-504           [-1, 1376, 1, 1]               0\n",
            "          Conv2d-505            [-1, 128, 1, 1]         176,128\n",
            "     BatchNorm2d-506            [-1, 128, 1, 1]             256\n",
            "            ReLU-507            [-1, 128, 1, 1]               0\n",
            "          Conv2d-508             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-509           [-1, 1408, 1, 1]           2,816\n",
            "            ReLU-510           [-1, 1408, 1, 1]               0\n",
            "          Conv2d-511            [-1, 128, 1, 1]         180,224\n",
            "     BatchNorm2d-512            [-1, 128, 1, 1]             256\n",
            "            ReLU-513            [-1, 128, 1, 1]               0\n",
            "          Conv2d-514             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-515           [-1, 1440, 1, 1]           2,880\n",
            "            ReLU-516           [-1, 1440, 1, 1]               0\n",
            "          Conv2d-517            [-1, 128, 1, 1]         184,320\n",
            "     BatchNorm2d-518            [-1, 128, 1, 1]             256\n",
            "            ReLU-519            [-1, 128, 1, 1]               0\n",
            "          Conv2d-520             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-521           [-1, 1472, 1, 1]           2,944\n",
            "            ReLU-522           [-1, 1472, 1, 1]               0\n",
            "          Conv2d-523            [-1, 128, 1, 1]         188,416\n",
            "     BatchNorm2d-524            [-1, 128, 1, 1]             256\n",
            "            ReLU-525            [-1, 128, 1, 1]               0\n",
            "          Conv2d-526             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-527           [-1, 1504, 1, 1]           3,008\n",
            "            ReLU-528           [-1, 1504, 1, 1]               0\n",
            "          Conv2d-529            [-1, 128, 1, 1]         192,512\n",
            "     BatchNorm2d-530            [-1, 128, 1, 1]             256\n",
            "            ReLU-531            [-1, 128, 1, 1]               0\n",
            "          Conv2d-532             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-533           [-1, 1536, 1, 1]           3,072\n",
            "            ReLU-534           [-1, 1536, 1, 1]               0\n",
            "          Conv2d-535            [-1, 128, 1, 1]         196,608\n",
            "     BatchNorm2d-536            [-1, 128, 1, 1]             256\n",
            "            ReLU-537            [-1, 128, 1, 1]               0\n",
            "          Conv2d-538             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-539           [-1, 1568, 1, 1]           3,136\n",
            "            ReLU-540           [-1, 1568, 1, 1]               0\n",
            "          Conv2d-541            [-1, 128, 1, 1]         200,704\n",
            "     BatchNorm2d-542            [-1, 128, 1, 1]             256\n",
            "            ReLU-543            [-1, 128, 1, 1]               0\n",
            "          Conv2d-544             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-545           [-1, 1600, 1, 1]           3,200\n",
            "            ReLU-546           [-1, 1600, 1, 1]               0\n",
            "          Conv2d-547            [-1, 128, 1, 1]         204,800\n",
            "     BatchNorm2d-548            [-1, 128, 1, 1]             256\n",
            "            ReLU-549            [-1, 128, 1, 1]               0\n",
            "          Conv2d-550             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-551           [-1, 1632, 1, 1]           3,264\n",
            "            ReLU-552           [-1, 1632, 1, 1]               0\n",
            "          Conv2d-553            [-1, 128, 1, 1]         208,896\n",
            "     BatchNorm2d-554            [-1, 128, 1, 1]             256\n",
            "            ReLU-555            [-1, 128, 1, 1]               0\n",
            "          Conv2d-556             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-557           [-1, 1664, 1, 1]           3,328\n",
            "            ReLU-558           [-1, 1664, 1, 1]               0\n",
            "          Conv2d-559            [-1, 128, 1, 1]         212,992\n",
            "     BatchNorm2d-560            [-1, 128, 1, 1]             256\n",
            "            ReLU-561            [-1, 128, 1, 1]               0\n",
            "          Conv2d-562             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-563           [-1, 1696, 1, 1]           3,392\n",
            "            ReLU-564           [-1, 1696, 1, 1]               0\n",
            "          Conv2d-565            [-1, 128, 1, 1]         217,088\n",
            "     BatchNorm2d-566            [-1, 128, 1, 1]             256\n",
            "            ReLU-567            [-1, 128, 1, 1]               0\n",
            "          Conv2d-568             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-569           [-1, 1728, 1, 1]           3,456\n",
            "            ReLU-570           [-1, 1728, 1, 1]               0\n",
            "          Conv2d-571            [-1, 128, 1, 1]         221,184\n",
            "     BatchNorm2d-572            [-1, 128, 1, 1]             256\n",
            "            ReLU-573            [-1, 128, 1, 1]               0\n",
            "          Conv2d-574             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-575           [-1, 1760, 1, 1]           3,520\n",
            "            ReLU-576           [-1, 1760, 1, 1]               0\n",
            "          Conv2d-577            [-1, 128, 1, 1]         225,280\n",
            "     BatchNorm2d-578            [-1, 128, 1, 1]             256\n",
            "            ReLU-579            [-1, 128, 1, 1]               0\n",
            "          Conv2d-580             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-581           [-1, 1792, 1, 1]           3,584\n",
            "            ReLU-582           [-1, 1792, 1, 1]               0\n",
            "          Conv2d-583            [-1, 128, 1, 1]         229,376\n",
            "     BatchNorm2d-584            [-1, 128, 1, 1]             256\n",
            "            ReLU-585            [-1, 128, 1, 1]               0\n",
            "          Conv2d-586             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-587           [-1, 1824, 1, 1]           3,648\n",
            "            ReLU-588           [-1, 1824, 1, 1]               0\n",
            "          Conv2d-589            [-1, 128, 1, 1]         233,472\n",
            "     BatchNorm2d-590            [-1, 128, 1, 1]             256\n",
            "            ReLU-591            [-1, 128, 1, 1]               0\n",
            "          Conv2d-592             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-593           [-1, 1856, 1, 1]           3,712\n",
            "            ReLU-594           [-1, 1856, 1, 1]               0\n",
            "          Conv2d-595            [-1, 128, 1, 1]         237,568\n",
            "     BatchNorm2d-596            [-1, 128, 1, 1]             256\n",
            "            ReLU-597            [-1, 128, 1, 1]               0\n",
            "          Conv2d-598             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-599           [-1, 1888, 1, 1]           3,776\n",
            "            ReLU-600           [-1, 1888, 1, 1]               0\n",
            "          Conv2d-601            [-1, 128, 1, 1]         241,664\n",
            "     BatchNorm2d-602            [-1, 128, 1, 1]             256\n",
            "            ReLU-603            [-1, 128, 1, 1]               0\n",
            "          Conv2d-604             [-1, 32, 1, 1]          36,864\n",
            "     BatchNorm2d-605           [-1, 1920, 1, 1]           3,840\n",
            "     BatchNorm1d-606                 [-1, 3840]           7,680\n",
            "         Dropout-607                 [-1, 3840]               0\n",
            "          Linear-608                  [-1, 512]       1,966,592\n",
            "             ELU-609                  [-1, 512]               0\n",
            "     BatchNorm1d-610                  [-1, 512]           1,024\n",
            "         Dropout-611                  [-1, 512]               0\n",
            "          Linear-612                    [-1, 2]           1,026\n",
            "================================================================\n",
            "Total params: 20,069,250\n",
            "Trainable params: 20,069,250\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 9.04\n",
            "Params size (MB): 76.56\n",
            "Estimated Total Size (MB): 85.61\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biqXF-r_TYDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_lr(optimizer, lr):\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = lr\n",
        "def update_mom(optimizer, mom):\n",
        "    for g in optimizer.param_groups:\n",
        "        g['momentum'] = mom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1pdxZ2Dl_F3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import sys\n",
        "def train(model,dataloaders,device,num_epochs,lr,batch_size,patience):\n",
        "    phase1 = dataloaders.keys()\n",
        "    losses = list()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    acc = list()\n",
        "    flag = 0\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr,momentum = 0.9)\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch:',epoch)\n",
        "        for phase in phase1:\n",
        "            epoch_metrics = {\"loss\": [], \"acc\": []}\n",
        "            if phase == ' train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            for  batch_idx, (data, target) in enumerate(dataloaders[phase]):\n",
        "                data, target = Variable(data), Variable(target)\n",
        "                data = data.type(torch.FloatTensor).to(device)\n",
        "                target = target.type(torch.LongTensor).to(device)\n",
        "                optimizer.zero_grad()\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "                acc = 100 * (output.detach().argmax(1) == target).cpu().numpy().mean()\n",
        "                epoch_metrics[\"loss\"].append(loss.item())\n",
        "                epoch_metrics[\"acc\"].append(acc)\n",
        "                lr,mom = onecyc.calc()\n",
        "                update_lr(optimizer, lr)\n",
        "                update_mom(optimizer, mom)\n",
        "                \n",
        "                if(phase =='train'):\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                sys.stdout.write(\n",
        "                \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f), Acc: %.2f%% (%.2f%%)]\"\n",
        "                % (\n",
        "                    epoch,\n",
        "                    num_epochs,\n",
        "                    batch_idx,\n",
        "                    len(dataloaders[phase]),\n",
        "                    loss.item(),\n",
        "                    np.mean(epoch_metrics[\"loss\"]),\n",
        "                    acc,\n",
        "                    np.mean(epoch_metrics[\"acc\"]),\n",
        "                    )\n",
        "                )\n",
        "               \n",
        "            epoch_acc = np.mean(epoch_metrics[\"acc\"])\n",
        "            epoch_loss = np.mean(epoch_metrics[\"loss\"])\n",
        "        print('')  \n",
        "        print('{} Accuracy: {}'.format(phase,epoch_acc.item()))\n",
        "    return losses,acc\n",
        "\n",
        "def train_model(model,dataloaders,encoder,lr_scheduler = None,inv_normalize = None,num_epochs=10,lr=0.0001,batch_size=8,patience = None,classes = None):\n",
        "    dataloader_train = {}\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    losses = list()\n",
        "    accuracy = list()\n",
        "    key = dataloaders.keys()\n",
        "    perform_test = False\n",
        "    for phase in key:\n",
        "        if(phase == 'test'):\n",
        "            perform_test = True\n",
        "        else:\n",
        "            dataloader_train.update([(phase,dataloaders[phase])])\n",
        "    losses,accuracy = train(model,dataloader_train,device,num_epochs,lr,batch_size,patience)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AITezIhUR8BM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OneCycle(object):\n",
        "    \"\"\"\n",
        "    In paper (https://arxiv.org/pdf/1803.09820.pdf), author suggests to do one cycle during \n",
        "    whole run with 2 steps of equal length. During first step, increase the learning rate \n",
        "    from lower learning rate to higher learning rate. And in second step, decrease it from \n",
        "    higher to lower learning rate. This is Cyclic learning rate policy. Author suggests one \n",
        "    addition to this. - During last few hundred/thousand iterations of cycle reduce the \n",
        "    learning rate to 1/100th or 1/1000th of the lower learning rate.\n",
        "    Also, Author suggests that reducing momentum when learning rate is increasing. So, we make \n",
        "    one cycle of momentum also with learning rate - Decrease momentum when learning rate is \n",
        "    increasing and increase momentum when learning rate is decreasing.\n",
        "    Args:\n",
        "        nb              Total number of iterations including all epochs\n",
        "        max_lr          The optimum learning rate. This learning rate will be used as highest \n",
        "                        learning rate. The learning rate will fluctuate between max_lr to\n",
        "                        max_lr/div and then (max_lr/div)/div.\n",
        "        momentum_vals   The maximum and minimum momentum values between which momentum will\n",
        "                        fluctuate during cycle.\n",
        "                        Default values are (0.95, 0.85)\n",
        "        prcnt           The percentage of cycle length for which we annihilate learning rate\n",
        "                        way below the lower learnig rate.\n",
        "                        The default value is 10\n",
        "        div             The division factor used to get lower boundary of learning rate. This\n",
        "                        will be used with max_lr value to decide lower learning rate boundary.\n",
        "                        This value is also used to decide how much we annihilate the learning \n",
        "                        rate below lower learning rate.\n",
        "                        The default value is 10.\n",
        "    \"\"\"\n",
        "    def __init__(self, nb, max_lr, momentum_vals=(0.95, 0.85), prcnt= 10 , div=10):\n",
        "        self.nb = nb\n",
        "        self.div = div\n",
        "        self.step_len =  int(self.nb * (1- prcnt/100)/2)\n",
        "        self.high_lr = max_lr\n",
        "        self.low_mom = momentum_vals[1]\n",
        "        self.high_mom = momentum_vals[0]\n",
        "        self.prcnt = prcnt\n",
        "        self.iteration = 0\n",
        "        self.lrs = []\n",
        "        self.moms = []\n",
        "        \n",
        "    def calc(self):\n",
        "        self.iteration += 1\n",
        "        lr = self.calc_lr()\n",
        "        mom = self.calc_mom()\n",
        "        return (lr, mom)\n",
        "        \n",
        "    def calc_lr(self):\n",
        "        if self.iteration==self.nb:\n",
        "            self.iteration = 0\n",
        "            self.lrs.append(self.high_lr/self.div)\n",
        "            return self.high_lr/self.div\n",
        "        if self.iteration > 2 * self.step_len:\n",
        "            ratio = (self.iteration - 2 * self.step_len) / (self.nb - 2 * self.step_len)\n",
        "            lr = self.high_lr * ( 1 - 0.99 * ratio)/self.div\n",
        "        elif self.iteration > self.step_len:\n",
        "            ratio = 1- (self.iteration -self.step_len)/self.step_len\n",
        "            lr = self.high_lr * (1 + ratio * (self.div - 1)) / self.div\n",
        "        else :\n",
        "            ratio = self.iteration/self.step_len\n",
        "            lr = self.high_lr * (1 + ratio * (self.div - 1)) / self.div\n",
        "        self.lrs.append(lr)\n",
        "        return lr\n",
        "    \n",
        "    def calc_mom(self):\n",
        "        if self.iteration==self.nb:\n",
        "            self.iteration = 0\n",
        "            self.moms.append(self.high_mom)\n",
        "            return self.high_mom\n",
        "        if self.iteration > 2 * self.step_len:\n",
        "            mom = self.high_mom\n",
        "        elif self.iteration > self.step_len:\n",
        "            ratio = (self.iteration -self.step_len)/self.step_len\n",
        "            mom = self.low_mom + ratio * (self.high_mom - self.low_mom)\n",
        "        else :\n",
        "            ratio = self.iteration/self.step_len\n",
        "            mom = self.high_mom - ratio * (self.high_mom - self.low_mom)\n",
        "        self.moms.append(mom)\n",
        "        return mom"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EShYnj0wV744",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "onecyc = OneCycle(len(train_loader)*5,0.003)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UnpKvf30_F3N",
        "colab_type": "code",
        "outputId": "3e4d5fc0-5b03-466f-ef29-f4f279997e22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import cv2\n",
        "lr = 0.003\n",
        "train_model(classifier1,dataloaders,encoder,inv_normalize = None,num_epochs=5,lr = lr,batch_size = batch_size,patience = None,classes = classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "[Epoch 0/5] [Batch 205/547] [Loss: 0.057643 (0.161759), Acc: 96.88% (92.98%)]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiBO2xUOtyRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in classifier1.parameters():\n",
        "  param.requires_grad = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig9IDc70U11d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "M8ZVPvu2_F3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "9ca34290-deba-42c8-d766-96a352123ad5"
      },
      "source": [
        "lr = 0.001\n",
        "train_model(classifier1,dataloaders,encoder,inv_normalize = None,num_epochs=5,lr = lr,batch_size = batch_size,patience = None,classes = classes)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "[Epoch 0/5] [Batch 68/69] [Loss: 0.018706 (0.008370), Acc: 98.91% (99.74%)]\n",
            "train Accuracy: 99.73515280403277\n",
            "Epoch: 1\n",
            "[Epoch 1/5] [Batch 66/69] [Loss: 0.001682 (0.012849), Acc: 100.00% (99.52%)]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-7d08c0120ea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minv_normalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-85-79c2eefff3ed>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, encoder, lr_scheduler, inv_normalize, num_epochs, lr, batch_size, patience, classes)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mdataloader_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-85-79c2eefff3ed>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloaders, device, num_epochs, lr, batch_size, patience)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 sys.stdout.write(\n\u001b[1;32m     42\u001b[0m                 \u001b[0;34m\"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f), Acc: %.2f%% (%.2f%%)]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayOENci9tQTh",
        "colab_type": "code",
        "outputId": "1593eebf-f71a-44e4-fc2e-a46230ee8b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "lr = 0.0005\n",
        "train_model(classifier1,dataloaders,encoder,inv_normalize = None,num_epochs=8,lr = lr,batch_size = batch_size,patience = None,classes = classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "[Epoch 0/8] [Batch 68/69] [Loss: 0.014240 (0.025348), Acc: 98.91% (99.08%)]\n",
            "train Accuracy: 99.08411113736611\n",
            "Epoch: 1\n",
            "[Epoch 1/8] [Batch 68/69] [Loss: 0.026409 (0.020939), Acc: 98.91% (99.27%)]\n",
            "train Accuracy: 99.27093178954001\n",
            "Epoch: 2\n",
            "[Epoch 2/8] [Batch 68/69] [Loss: 0.004784 (0.026049), Acc: 100.00% (99.20%)]\n",
            "train Accuracy: 99.20176630434783\n",
            "Epoch: 3\n",
            "[Epoch 3/8] [Batch 68/69] [Loss: 0.048686 (0.019452), Acc: 98.91% (99.39%)]\n",
            "train Accuracy: 99.38981765910523\n",
            "Epoch: 4\n",
            "[Epoch 4/8] [Batch 68/69] [Loss: 0.017905 (0.017236), Acc: 100.00% (99.48%)]\n",
            "train Accuracy: 99.48482789855072\n",
            "Epoch: 5\n",
            "[Epoch 5/8] [Batch 68/69] [Loss: 0.001604 (0.017246), Acc: 100.00% (99.48%)]\n",
            "train Accuracy: 99.48482789855072\n",
            "Epoch: 6\n",
            "[Epoch 6/8] [Batch 68/69] [Loss: 0.013639 (0.056296), Acc: 100.00% (99.34%)]\n",
            "train Accuracy: 99.33763586956522\n",
            "Epoch: 7\n",
            "[Epoch 7/8] [Batch 68/69] [Loss: 0.002829 (0.025053), Acc: 100.00% (99.22%)]\n",
            "train Accuracy: 99.21875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MTWZeH6N_F3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class cactus_dataset_test(Dataset):\n",
        "  def __init__(self,image_dir,transform = None):\n",
        "    self.img_dir = image_dir\n",
        "    self.transform = transform\n",
        "    self.id = os.listdir(image_dir)\n",
        "  def __len__(self):\n",
        "    return len(self.id)\n",
        "  def __getitem__(self,idx):\n",
        "    img_name = os.path.join(self.img_dir, self.id[idx])\n",
        "    image = cv2.imread(img_name)\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "    return (self.id[idx],image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9sOL3im7_F3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test1 = cactus_dataset_test('/kaggle/input/test/test',test_transforms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ca9-Vjgt_F3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loader = DataLoader(test1, batch_size =32, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YgdgWGGz_F3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model,dataloader,device,batch_size):\n",
        "    running_corrects = 0\n",
        "    running_loss=0\n",
        "    pred = []\n",
        "    id = list()\n",
        "    sm = nn.Softmax(dim = 1)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for batch_idx, (id_1,data) in enumerate(dataloader):\n",
        "        data = Variable(data)\n",
        "        data = data.type(torch.FloatTensor).to(device)\n",
        "        model.eval()\n",
        "        output = model(data)\n",
        "        #output = sm(output)\n",
        "        _, preds = torch.max(output, 1)\n",
        "        preds = preds.cpu().numpy()\n",
        "        preds = np.reshape(preds,(len(preds),1))\n",
        "        \n",
        "        for i in range(len(preds)):\n",
        "            pred.append(preds[i])\n",
        "            id.append(id_1[i])\n",
        "    return id,pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS50G7gitxA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4ASL-6ej_F3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "id,pred = test(classifier,test_loader,'cuda',32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tD3JH64u_F3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = list()\n",
        "for i in range(len(pred)):\n",
        "    a.append(pred[i][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8z92Xwpu_F3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = np.asarray(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "I2kKxc4C_F3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = np.reshape(a,(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JyGmiWxi_F31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = np.asarray(id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "g4Vg43vD_F33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = np.reshape(b,(-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "v7VljxO4_F36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub = np.concatenate((b,a),axis = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VzqPc8TR_F38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub_df = pd.DataFrame(sub)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BkNawYCf_F3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub_df.columns = ['id','has_cactus']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "IU5HSD6m_F4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "M8KKa_Q0_F4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub_df.to_csv(\"/kaggle/working/submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}