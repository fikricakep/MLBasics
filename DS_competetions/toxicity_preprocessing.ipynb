{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "toxicity_preprocessing",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanchit2843/ds_competitions/blob/master/toxicity_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZNDSAOKBwAO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d3ebd8ba-e449-4e1a-9f8f-0b6142abeb33"
      },
      "source": [
        "\n",
        "\n",
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "\n",
        "import os\n",
        "# #print(os.listdir(\"/kaggle/input/test\"))\n",
        "\n",
        "import torchvision\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import os\n",
        "import torch\n",
        "from tqdm.autonotebook import tqdm\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "# Any results you write to the current directory are saved as output.\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "tqdm.pandas()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWXh_BelCKn6",
        "colab_type": "code",
        "outputId": "61edfc5b-036c-4ccb-ea10-f635278121b3",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "\n",
        "\n",
        "!pip install -i https://test.pypi.org/simple/ supportlib\n",
        "import supportlib.gettingdata as getdata\n",
        "getdata.kaggle()\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://test.pypi.org/simple/\n",
            "Requirement already satisfied: supportlib in /usr/local/lib/python3.6/dist-packages (0.1.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dc6230a7-2b59-426b-abe4-b679e9bbb051\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-dc6230a7-2b59-426b-abe4-b679e9bbb051\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zxbkSSUCO5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## adding data and unzipping it\n",
        "# !kaggle competitions download -c jigsaw-unintended-bias-in-toxicity-classification\n",
        "# !kaggle datasets download -d chriscc/pickled-word-embedding\n",
        "# !kaggle datasets download -d authman/pickled-glove840b300d-for-10sec-loading\n",
        "# # !ls\n",
        "# getdata.zipextract('/content/pickled-glove840b300d-for-10sec-loading.zip')\n",
        "# getdata.zipextract('/content/pickled-word-embedding.zip')\n",
        "\n",
        "\n",
        "# getdata.zipextract('/content/test.csv.zip')\n",
        "# getdata.zipextract('/content/train.csv.zip')\n",
        "\n",
        "# !ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tE94CuADdrw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aea9650c-d201-4d80-b63f-9c22d8a92724"
      },
      "source": [
        "## for importing data\n",
        "train = pd.read_csv(\"../content/train.csv\")\n",
        "test = pd.read_csv(\"../content/test.csv\")\n",
        "print(\"Train shape : \",train.shape)\n",
        "print(\"Test shape : \",test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape :  (1804874, 45)\n",
            "Test shape :  (97320, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fjSPC-PDwLO",
        "colab_type": "code",
        "outputId": "0741e04f-147f-47dc-f820-4aeda41f0d33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def build_vocab(sentences, verbose =  True):\n",
        "    \"\"\"\n",
        "    :param sentences: list of list of words\n",
        "    :return: dictionary of words and their count\n",
        "    \"\"\"\n",
        "    vocab = {}\n",
        "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                vocab[word] += 1\n",
        "            except KeyError:\n",
        "                vocab[word] = 1\n",
        "    return vocab\n",
        "\"\"\"\n",
        "def build_vocab(sentences, verbose = True): \n",
        "from collections import defaultdict\n",
        "\n",
        "vocab = defaultdict(lambda : 0)\n",
        "for sentence in tqdm(sentences, disable = (not verbose)):\n",
        "for word in sentence: \n",
        "vocab[word] += 1\n",
        "return vocab\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# sentences = train[\"comment_text\"].progress_apply(lambda x: x.split()).values\n",
        "# vocab = build_vocab(sentences)\n",
        "# print({k : vocab[k] for k in list(vocab)[:10]})"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef build_vocab(sentences, verbose = True): \\nfrom collections import defaultdict\\n\\nvocab = defaultdict(lambda : 0)\\nfor sentence in tqdm(sentences, disable = (not verbose)):\\nfor word in sentence: \\nvocab[word] += 1\\nreturn vocab\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lgLg7fcD42Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set embedding layer path :\n",
        "\n",
        "# crawl_path = '..../content/crawl-300d-2M.pkl'\n",
        "glove_twitter_path = '../content/glove.twitter.27B.200d.pkl'\n",
        "# glove_840_path = '..../content/glove.840B.300d.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JQBYOxbRdVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "we have imported pickle file so. \n",
        "\n",
        "Of course we also need to adjust the load_embeddings function, to now handle the pickled dict.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "## if we use .bin form of embedding\n",
        "# from gensim.models import KeyedVectors\n",
        "\n",
        "# embeddings_index = KeyedVectors.load_word2vec_format(glove_twitter_path,binary = True)\n",
        "\n",
        "import pickle\n",
        "\n",
        "NUM_MODELS = 2\n",
        "LSTM_UNITS = 128\n",
        "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
        "MAX_LEN = 220\n",
        "\n",
        "def get_coefs(word, *arr):\n",
        "    return word, np.asarray(arr, dtype='float32')\n",
        "\n",
        "\n",
        "def load_embeddings(path):\n",
        "    with open(path,'rb') as f:\n",
        "        emb_arr = pickle.load(f)\n",
        "    return emb_arr\n",
        "\n",
        "def build_matrix(word_index, path):\n",
        "    embedding_index = load_embeddings(path)\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "    unknown_words = []\n",
        "    \n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            embedding_matrix[i] = embedding_index[word]\n",
        "        except KeyError:\n",
        "            unknown_words.append(word)\n",
        "    return embedding_matrix, unknown_words\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "class SpatialDropout(nn.Dropout2d):\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
        "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
        "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
        "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
        "        x = x.squeeze(2)  # (N, T, K)\n",
        "        return x\n",
        "\n",
        "def train_model(learn,test,output_dim,lr=0.001,\n",
        "                batch_size=512, n_epochs=4,\n",
        "                enable_checkpoint_ensemble=True):\n",
        "    \n",
        "    all_test_preds = []\n",
        "    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n",
        "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
        "    n = len(learn.data.train_dl)\n",
        "    phases = [(TrainingPhase(n).schedule_hp('lr', lr * (0.6**(i)))) for i in range(n_epochs)]\n",
        "    sched = GeneralScheduler(learn, phases)\n",
        "    learn.callbacks.append(sched)\n",
        "    for epoch in range(n_epochs):\n",
        "        learn.fit(1)\n",
        "        test_preds = np.zeros((len(test), output_dim))    \n",
        "        for i, x_batch in enumerate(test_loader):\n",
        "            X = x_batch[0].cuda()\n",
        "            y_pred = sigmoid(learn.model(X).detach().cpu().numpy())\n",
        "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n",
        "\n",
        "        all_test_preds.append(test_preds)\n",
        "\n",
        "\n",
        "    if enable_checkpoint_ensemble:\n",
        "        test_preds = np.average(all_test_preds, weights=checkpoint_weights, axis=0)    \n",
        "    else:\n",
        "        test_preds = all_test_preds[-1]\n",
        "        \n",
        "    return test_preds\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cryY6VOQSEQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ## TODO:\n",
        "# ## find and remove sprecial characters \n",
        "\n",
        "\n",
        "## Don't run this\n",
        "\n",
        "\n",
        "# def bad_preprocess(data):\n",
        "#     '''\n",
        "#     Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
        "#     '''\n",
        "#     punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
        "#     def clean_special_chars(text, punct):\n",
        "#         for p in punct:\n",
        "#             text = text.replace(p, ' ')\n",
        "#         return text\n",
        "\n",
        "#     data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
        "#     return data\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VENrvC0U3ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator \n",
        "\n",
        "def check_coverage(vocab,embeddings_index):\n",
        "    a = {}\n",
        "    oov = {}\n",
        "    covered_word_count  = 0\n",
        "    oov_word_count  = 0\n",
        "    for word in tqdm(vocab):\n",
        "        try:\n",
        "            a[word] = embeddings_index[word]\n",
        "            covered_word_count += vocab[word]\n",
        "        except:\n",
        "\n",
        "            oov[word] = vocab[word]\n",
        "            oov_word_count  += vocab[word]\n",
        "            pass\n",
        "\n",
        "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
        "    print('Found embeddings for  {:.2%} of all text'.format(covered_word_count / (covered_word_count + oov_word_count )))\n",
        "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
        "\n",
        "    return sorted_x\n",
        "\n",
        "# Side Note:\n",
        "\n",
        "# It seems that k += vocab[word] will only be run if word can be found in embeddings_index,\n",
        "# otherwise i += vocab[word] will be run. So they are not the same. \n",
        "# You can actually replace try-except with an if-else.\n",
        "\n",
        "# Also, vocab seem to store the frequency of a word in the corpus,\n",
        "# so I think k and i represent the number of words covered in the corpus."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C1eMGQbcq1H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3400bff2-4969-480d-e25b-9d2d89db294a"
      },
      "source": [
        "# oov = check_coverage(vocab,embeddings_index)\n",
        "import time\n",
        "tic = time.time()\n",
        "## lead and take note here\n",
        "\n",
        "\n",
        "###\n",
        "glove_embeddings = load_embeddings(glove_twitter_path)\n",
        "# loaded 1193514 word vectors in 7.11469292640686s\n",
        "# Found embeddings for 4.22% of vocab\n",
        "# Found embeddings for  74.85% of all text\n",
        "\n",
        "\n",
        "print(f'loaded {len(glove_embeddings)} word vectors in {time.time()-tic}s')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded 1193514 word vectors in 6.848312854766846s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dXKVc8Tc4Nc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "6b457b4c-48e9-4258-c512-ce2eb6905770"
      },
      "source": [
        "vocab = build_vocab(list(train['comment_text'].apply(lambda x:x.split())))\n",
        "oov = check_coverage(vocab,glove_embeddings)\n",
        "oov[:10]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1804874/1804874 [00:29<00:00, 62149.06it/s]\n",
            "100%|██████████| 1670966/1670966 [00:03<00:00, 529971.59it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found embeddings for 4.22% of vocab\n",
            "Found embeddings for  74.85% of all text\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 861783),\n",
              " ('The', 435047),\n",
              " (\"don't\", 178881),\n",
              " ('Trump', 156956),\n",
              " ('It', 153815),\n",
              " ('You', 144381),\n",
              " ('If', 143987),\n",
              " ('And', 128132),\n",
              " ('This', 121363),\n",
              " (\"it's\", 100959)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l2iLlEDfrzV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "dd868ee4-660a-4bca-97f5-fc32709f44ec"
      },
      "source": [
        "import string\n",
        "latin_similar = \"’'‘ÆÐƎƏƐƔĲŊŒẞÞǷȜæðǝəɛɣĳŋœĸſßþƿȝĄƁÇĐƊĘĦĮƘŁØƠŞȘŢȚŦŲƯY̨Ƴąɓçđɗęħįƙłøơşșţțŧųưy̨ƴÁÀÂÄǍĂĀÃÅǺĄÆǼǢƁĆĊĈČÇĎḌĐƊÐÉÈĖÊËĚĔĒĘẸƎƏƐĠĜǦĞĢƔáàâäǎăāãåǻąæǽǣɓćċĉčçďḍđɗðéèėêëěĕēęẹǝəɛġĝǧğģɣĤḤĦIÍÌİÎÏǏĬĪĨĮỊĲĴĶƘĹĻŁĽĿʼNŃN̈ŇÑŅŊÓÒÔÖǑŎŌÕŐỌØǾƠŒĥḥħıíìiîïǐĭīĩįịĳĵķƙĸĺļłľŀŉńn̈ňñņŋóòôöǒŏōõőọøǿơœŔŘŖŚŜŠŞȘṢẞŤŢṬŦÞÚÙÛÜǓŬŪŨŰŮŲỤƯẂẀŴẄǷÝỲŶŸȲỸƳŹŻŽẒŕřŗſśŝšşșṣßťţṭŧþúùûüǔŭūũűůųụưẃẁŵẅƿýỳŷÿȳỹƴźżžẓ\"\n",
        "white_list = string.ascii_letters + string.digits + latin_similar + ' '\n",
        "white_list += \"'\"\n",
        "\n",
        "\n",
        "glove_chars = ''.join([c for c in tqdm(glove_embeddings) if len(c) == 1])\n",
        "glove_symbols = ''.join([c for c in glove_chars if not c in white_list])\n",
        "glove_symbols"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1193514/1193514 [00:00<00:00, 2241347.75it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'.:,!\"?()！。-、…/*>^？<・&♥“”_♡´،~;（･|[）]—笑ω❤～★`$و♪=｀¿+☆☺ﾟ▽%✔@؟•█｡в\\\\；¡＾／→∀：＼°и←⌣#я}{░д✌˘¬━–сﾉه⭕»☀あ☹＿月✅➊＞«⌒＜●∇а➌➋え♦➍♫─○ノ✋‾▀日▄؛═，➎́๑┓↘✨✈⇒┃ㅋу̀ヽ∩×≦≧ーب↙‹┌ε▒＆＠☝║☯―σ┛▿円ｗ┐£о›｣≡んي┗位ƪ▓┏｢☑►ʃ┈فⓜ╗↓っم❄♬年̷ع時╯三＝╰､※╔·は■％｜◎ˆ∠＊☞⊂✰к아お人◦╮◠€̮‿▂з╚➡と╭⊃╝ゝ◡˙º□❌└©↑☜で⊙．艸¯◇╩̩➏¼＃╦ヾ☻∧▼－回ㅠ◆╱✿いۈ이➒◔♻✧◞그＋◉˚∂일수╬╲のつ̯◟╹لｏ⛄더ิ난┊第が分²▬➐✊❗다왜내⁰∞나ا￥┻なね⁾ง®ˇ☁ⓔ泣¸┘╥ゞ͡╠ﷺ✓①˛¨−잘월か⚪⚡②◕ｰ┳щ۳私う좀［を］▇ﻻ☔▸╣³⚽̶⭐̅՞ま♩✳今❀౪◄ⓣฺु△ۆ∵♂한◾۶▅√θ†▔◯すⓞ✖⚾′俺™土것੭안☼❛③ηツ▲❁℃ㅎ✩저시ο헐또전➜♔┣◝٩にｔ彡▶歳◜金▐년や✗♛へᴗ♣◍☕だ▕ª͟し⚫¥❥네わ¤제‸뭐ㅇⓡ⇦╋ヮ本할ふ│④중❶пこ̥ⓦ♠ⓛ¦▾❷そ❸た☎‵▉ロ枚┫✴☉卍سも꼭さ분ฅг水⇢⁽ﷲ▌때ㅁ▏⇠よ✪응٣ξ❹음ك┼ρ♉와話◢نㅅدｉ❒☂汗ｪ爆☛참て₍٢∪┎⇄١의◤خू너ｖॢ⚠♕ً♏날٥１рェ가ๆ❕거м≫ザ⇨ـ§는木⌓ﺑ어건„✉⠀嵐↗ッ✽ɔ에위ㅂ火ⓕღ☃ຶ☐✂⑤͈를ʔਊ∴을◒く하るㅜ♈막∑⋆₎◀오후男◣♀┒≪３点万게▷명丿ψㅤ涙２▪е못ت̫ミ٤ืأ번ლⓨき中ν約女ⓐらжれ⛅해ةх등말ア̵원̸┉❣ム♯̃◐➫개।◥号ัンむ№∫⑥대０✦бص½イ±✡ﻭ倍장僕≖個φｑ何ⓟ巻κ۞づみ˖͒ﾝʘقι⊖ⓘ▆นぁ₃넘ﻟ夜α΄♚ち넌⬅고큰ٰ⬇야◑ʕ¢ど嘘り⇩ⓤ♧壁ر″ﻵ✏仮๏걸로н≈̴될ِ∈件☇두̾黒만มｙぇ자ｂ猫ถｐ은♍‚⑅ٍ度秒ё٪❓照첫줄حཀﾍ部∗➙全✹걍ⓒﾛ♌▃ヘａ☣求朝ԅ祝⌯☮母➬헉高☾☋赤앗⇆т곧뭘↖皿ㅡ본유名살딱ラほ회밤譲♨і二δ∋̣лَζ番◼め工他♓جｃ̪愛ڡワ및듯フч볼␞エ＂눈➑차새ひ̐⛳じメش집☚✮✘ⓑｕ一ス恋◻↔ﾞ弾된٧後٦昼ⓚ♋暇\\u06dd⬛⇓⇛４⚓늘禁인도총돈５세誰代神✍ⓗ✞ご┬올怒ク님せҧ⬆ⓢวｍ¶＇니青형÷凸널̿けณハレй☄⛔ۚℓオ짱✯色小ⓝ으π➰٠ํべ妹ゆ➪╙⑦⏰온昔ドэ新著กິル夢ﾐ父\\x80◓ｱอ無犬٨✝リ❖棒۔死ﯙ✫↪ろﻓ∥❝ھ̆ⓙآ┴大μ글ｘ화楽ُː➔╖몇̲白▁ㄷஇ⁼⌑✤勝ʅｴ✲ｧ☪友ө억ϟ⍢⛽υ‡緑↷マ遊٭➭주เ밥⊱ю✒∬シ雨╓☟ｊ✾♒☠손ີ訳☽❔함➤⇔ぬｯ흠단ી╜지✺トば๕ۖ\\x94↺☸期⇚ّ϶種∨君ؤ\\x93真言計希❉⇱７า雪億ⓓ黄갈皆ﾌ✱出̊⋈６♢❅┆ء弟요➮８デ川９逃翔乂⑨↯≠対╤예웅⑧コض｛점❃⁄끝⛵上알흡ή✄台ۉ마ぉ┋⌚ृ과별돼복책⊥カ호曲紫♭系기ㄱ비사発박前階♊超⊿⌂꽤넹勇약➨흑ぅ春続☊ෆ⋃子ᐛ⬜❍ﻬᐜ株ф箱妄옷إ◌ｇ花ยτλ左둘ﾆｓ☏ۅ車星낼ط초ו次금ぷ罒◁ْ₊˃속ܫ夏娘☰⋰キ♤➹右்흥◈봐ח◙タ米票⁺げぼ⍛꿈列口ⓥ조♐❺╨✵울배組服⁀є매❊문➷↶ダ黙謎チ☢問♎˂袋ジگ뭔➕∙◘부敗▨肉ホ元ҩ쓴키様北ชہｄऀウﾗﾒ週⅜目丶ଘㄚ生〒ㄴﾚ푹違〆食⇣팬✬灬下ｚ\\x92顔¹✭∆봇간준↩ぃ❇ᾥ➖ۊｌ才⇜✸젤절兄▎光選̗뒤⌘ร例ᵕずバی歌的휴넵╞ш姉ゃ╡ｅ｝❽海⁂手ﾊ┤⛪≽ぐ씨↴ぜɷ≼ᕗ乙쪽├غ₌잠桜✶⇧봄ニ엉乇我땐風飯ค山͐↫パぶ頃➼⤴⋱๓ﻌ목ϖ쫌字八⍨반↳모적ꇤ힘\\x96森♑◊画ƒγ급길ㅗ➘⛺ป≀版藍ˋบ술탄데ｒ여편ᕕ฿≤න美割明癶条우권益⌇ｲㅈナϛ물↱⍤♞ｋˊ라伊⇙✷토ゴ＄宮≥쓸¾곳ɑٌ달뿐略힝サز☌泊몸文허셋初서➠說ガไ확哈웃딸❆┇▫好각自폰✼❂천匹数ởテ✆❜屮↻各머≒暗セァⓖ곡͛정￦再冊秋足定声冬ﻷ탑헤뿅잉ぞ完空ｎ殴ぽټ銀ꈊ미ｼ心家びע입❢➥彼旦층꺄⑩∿ʊ덜雷東報ㅏ从冫会✎ɞ髪흐ㅍญ남↲◽限❦헿̠ブ嫁긴ॣذ➲ぱ♿ぎプ客선쭉ｵ̼훗➽和신산型区발粒英ネ帰앞卐酒굿❈보☍빵채♘열진\\x97天体کｈ西법ӧ무캬戦ด애⚈ᆺฟ❞▰♜히間横⇉ծథಠᗨ殿➟聖༩等▊설刃早표ᴥ현엇速얘安കソจแモ福⌄囧ざｆ짝╳ױ동▍注幅親ڼ絵ะ有방ส됨ଓ世악ﾙ군頭✻￡連척┅茶ใ▵▋❋虎들ｩ編╢က外즉実ѡ℮β休김作級章着상״∮┰╫영鬼改ੈヤ바靴∘節⑲न▹集핳ケﻡˍظょ크◗学姫ව凛変당양智南╟त랑❙리形☒ห코င見┄力ㄟ指꽃嫌⁻➣社⏳疲며卵教순不ぴ❑줘\\x91∕‖⑬桃읭놈行田욕짤⌔◖ʚ⚗音✚⤵夫厨腐広王팀壊͜낮맛တ布ث思気ۗﺏ덤ى国宣眠ϵ˄ｮポϋ魚嬉ォ情구ಥ란է귀⍩⏩맘⑉图終팔ර尚ﾘ妻塩ボ공駅♗맨蘭र성ක̑翼⊰⊇▩凵◂萌廃✕≝馬ｷ♖မఠล咲생優꺅강앙迄正ԓ종寝了✠⌛ピ柔℡用芸告緩छ幸篇먼큽段멍鍋╻鍵味ќ裏든⇋↝˜엘ૂ있伝軟엥노๐付בต콜∽潤헝ΰ♙๋玉ヅ윽品ද千↕強쇼椿鳥ヒན▣끙ｨ잭繋ම虹┯소原胸피面絆۩ϑ役ȏ잔健⓪풉ъසပ핫谷⋯龍知蓮➿ஐ❧葵규✁缶悪長怖̓怠왕梓ィ암ｽ롤朴꽉쳇⏪പ뀨⚰នរ불公現쩝丸⇗͙˒써➳감歩존цﻑ·∏▦祭ｳɐ京導平当♟弱↬ท병ข₀ﷻ답杯性엣炎故∃否牛紅뙇零άﮩ딴グ❎ビ唯면起ό승乁엌兎柏可動골⍣앱ы感剛타ㄏέ解되喜闇왠ꇴ스松색˵외∼増↧♝พ☤氷✟益内村先ズ판ゲ˹ｫ엔낸လ豚⇝凹̋☭얍ɪලゎ춤ົ없ヨॄ雅තㅣぺ⌗걔☬震똥店┖誠നﾄﾀ┑໒͂ﾂ홀ក道亮ത┠吉辛⇘ギ腰枠큭主⑪첸鼻ˉ錠♺ユ질트≘鬱‰石ㄹស要旧렙響뜻遅望五연韓ｸ鏡땅쟤✜썰林∟헛┸옹⍸四ｿॐχ熱◩ں४ㅑ拡首ゅ삶垢⇊↵両柳ടโٓל薮싹짐逆良╧ꈍ냥린짠빈校ය焦↹ί通백ㆀ⍥˓鯖ƭ藤쳐恥耳豆命ʋ׳↰痛寒파腕渚薬컷ͦ楓血素짓̤☓ஜ드န彩鉄仏ﾅប敵狼旭뚝헣축̜පត題運ㅉ合ა猿ג決影ה티事ﻥ⊆毛ရ捕蟹투兆북鈴答킁풀옆ﾋॉម古⑫⋛눝͔왈쾅井˶첨野氏理⇇⊹亀最独薫⚙脱ャസ앤➂빠홈電찬ゥ包ٱ헷以表측残⓿在啊뻔傘ﺓ線開퍼若ɵペ腹七展門物岡即図핡경魂樹乳침८겸앜옴又힣͞意♱葉넴ạ˼⇈갑雑ٹǟь∝치仁直同친席式သ棗柱॑루⇐̄ㅆ結컴相散엄塾돌빛굳좋임ﾎ➁৩倉ﾖ梅虫께⊶肩￠೨‗ﻱमඉ쉿⇑ℱ畳ល⋌ﾑ떡滝역ў忍븉ベћᵒ悲嗯ヴ뉴ઌ숨岩入ପ草왱破평쯤통ﻉ▴項笠芋穴県枕奏杏那ƍ便ﾜග縦橙殺態ὢ脳재驚噂꾹糞々尻ؖ쌤嶺죄市⋅✙⣿깔脚ಲ⋋遙계빅携梶ש輪橘ওහ갓舞ꉺ움ⓧ憂礼淳竹커蒼ㅊ워呪쿡랩✥땀投흰➸太놀ぢ歯減̳˻切毒आㅓ與地城̻ට혀単重옛ﻝ민१둔ֆ॓헠ﺍ⇅✇司ခ錦┝剣̬탈来운詩♮奴敦‣絶웬톡魔॔혹ョ까梨ۙﷶゑכ➃칼업מ遥熊ڤ渋े⓵쏙➀経┍信周헙甲똑ㅌי十⓶是⊷書送綾伏얼창✐ҋ願힛칫苺ನ⓷⓸콩↜紺綿旅球灰⋚ზ카ӥ某鮭땡̇쿸엑ｶ岸ര용꼴റ佐聡⑴麺軍✣➄ᴈ닭≣้➅特최孫≍流極국ㄋ沙悠↭디降鹿怜ತѕµ骨羽政뜬ৣ쑻乾羊ｻ뻥ﺎ河雛따쌀廿송⑵؈涼ヲˈｺ떠쯧橋満桂↨같方근┨॥≋≻頁ㄲ검ુ망ರ偽修泉ͧ罪╪↠菅포化澪딩ђא๖瞳関칠ﻮစ島심퍽六┚⚲想病菊⒪⊕͏๔別➚엩ﯾૈ法甘➱蚊않ទ쿨ﾏ勢桁晴쉴패̡凶尾식홍˺華暁陽▱벌▻ﻣ屋達ℜ昴坂鳳누귤吧孝角풋ੇ普갤펌๒民씩צ場砂☥雄船ϊ檢⛲郁技ದ参裕숲攻͠꿀୨관律竜純沖寺ƌ∣ㅔ紙❘成۬悦໊乱波흙냐┥ﾁ唉税受裸많အ훅ュ곰ۀ寿都ར業窓̎露ല▢ູ尊者실翌隻⚑多格ﺳ栄ងᴖពͽ삼ﾃ兵⊛陸検読่隆ϱಕ激唇싶싼슝ѽ╸巨읏詞眼ച守雲➩燐ⓩ╘̂∰ㆍ언쫙➧⠤銅렌隣派由⑭ͼ柊շ닉喉幕墓少썅充˳爪청ฮ뷔➴始ธɜ輝ΐ➆童ۜ옙석秘栗∅快슈ɾﾔ▭줌큼半餅吐ﺂ죠ﮪ武亜캐急ㅃี⚐빚◃能ʬศѯ∻훨බʞ反萎叶ৡ魁ᵔ州학̛⎠⏬説圭静囗漢ᵘ┷點❐꺼欲⏝≎⑮狐씀얜째飴헹類机狂ƺ並勘℘ㆆ扉警⊝写╄支짜졸ဆ機加茜豊✃죽레⚆궁ᵌ웹͋യ∺ច≕핰⇖塁貞冠誤转톰你ซ╛待⒧♁街个덕យ탁۰팁תฬ❚界냠ﾈ추ಸ士핀ヌؔ̓浩愉항⑶녀빂툭▧౩≢ỏ援縁⌢鞭ʾ윤ۋ캡ゼ恵久ꏿ哀走香독⚬턱只板놔┕革彳멀蛇১넷⇡滅痔近샤霧買骸ｦ座櫻펑狩清眞탓油류폴櫂囲ដ進욱탐털폭富ಗᐖ豪消弓貴출⊚秦┿샘咳祈糸⌐੯뀽행博売섬์試찡⑯₂罠幽걘푸లជ臣証微썸흫넬۵빌折毎徹棚꽝利ॱ帝彰켄დ⒡戯悩➝引志❡未낀ۄ避常留쨘작⑸基鷹塚覚필ﾓ鉢得雫캉팩堺洋큐ɩ哦ヶ乃ﺗ래❏삐완⚭អ♆苦銃室朔宴劉侍즐戀킄質닌団엿Ꭲ局夕놉奥⊜락槍پ胃造វⁿ핵Ꮼ予콕深鶏쟝募ﺕ╕३ས瀬낄颯ᄇ૮ㅐ戸池רʿゾ蔵훈⊡팍菌≓立爺맥根처爽쥐썩פ⋂펜특然ਉ慶丈⅛둥髭वᄏ膝ڪ源텅尸粉ℳ쏘͵蝶왓津床杖℠刀▥所ʻ༡蛍使差像靑컵顎담메↞落➞택ﾇ່湊身語迷壱ώｬ路ۇ⒲⋤確➢商咚ʏ☈鳩活爻♰麻済뽀序柄끗뇌ϐ☨체ừ⍘怪蝉터永격将麗껄鞄鰻ထ벽百瞬ℒ⒟李ﻩ呆봉퓨ㅛฯ岁ї鶴Ꭴ혁ผ붕콘均玲̏軽ϡ엠姿탕픽ಯ➛☩念介屑╒톤ཬ哇江ͺವ恩↼클합୧蹴準덱凪突兼́ɯ幻➶案̀傷⋥隼ಮ飛싸匠斬椛低힐非ᐢờ獣冥ඩև༤৯러ㆁ캔断義ꇵ厂町柿ʎ센时황➓善ယ킹撃೫똭칸酢霊且唄벨舌흣創뜰弐必팸治˟➗噗宜⒜叫ꉂ背먹婿応➻栞館忘放⚜喝버銭⑰ए↢킬ձ碧副십ད量팡ɢ汝冷ɿ仲御씁ʡ燕盾뿌也勤≺似九札乗藻層ꆚヵ致뜸➦壺科臨겜⚣額쿵했負➺越⌨遠ֻ२벱읍철⚛➇끼려鴉ᗜ曜셤맞മ承멘갭끵器弥교픎秀丁쿠ಌ脇転⋮໐ꜝ担ﻹ論⍪堀寂庭総卒➈仕誕쫑앎뺨던辻ញ軒빼⚅팝ខ็翠ളគ콱∖坊景酷⚄暑对⑱ﻋ᾽共族止⚂築➵ⓠ▤ᗢ႔仝휙ﻶག꼬☦焔변帯亅抱瓜̽駆環프깬旬⦿컥잇쩜ᵎ印ז択坪ಪ쭈汚ਚ⋎煌里屁藁ဖ⏎瓶汁셈❻향◅兜엞엗癌哎ဘ뷰ﮨ茂૭聞鋼욜쟨བ描플끈˝ㄸ難͕ۨ打宿芝医遼떽宙벤紬過蜜착밀⒤持켁ો⚃⛎呉紘制활ʓ꿔뺀쩔품没盆⏫녜̦犯研푼昭ד阿ｹ져懐異링ﭑ⑷ඹ保謙ﻙ罰鳴肌袴果샷ゔ케比渡詳ꂧ뒷宝୭灯頬ੌ皮ﺀ課ꐦ産깨ഗ澤태寮롱융侑╃겁Ꭿ爱鮪빖ហ歴밑啦拓ﻫ℉앟ﻳ찍弦뭣浜杉率✛覇仙凄苗俊ֵ۫倒及岳提桐➉룰⍟屍幼返‟到뱀貝ǘ悶⇁ǚৎﮬฉണ☫∎核톱̚潮稲財범붐샹๊๗⋗勉츄팜向許এ廉ㅕ厚麦航힉뼈⋖멋ئ턴ᐟဒ凌돔ᕤ萬ꐕ升為ゐ鴨ѻ⒴盤퀸ﯛɴ朱딜읔⌦논ὤ准矢徳밖⒭➾泡빰至⅝調⁴囚극菜킥ಡ禿빨轉ﮮ忠精ﻕǁ慧꾸ᵃ史값͇拳팅弘玄⅔겟党☱쿤ಬ張ﺱ╵맵曇亿員幹엏勃諏록⋇⚀笛혼ﺁ잼ǥ꺆느⇲典̈́ཅ뛸\\u0602透햐ʖ湘说ړਕ演困陰햄ے操몰辰ɭտធ就ế細럽̭⇶朧陣르ℯ談징깸쿄登윗좌₤셀ﺹꃩ결탬런빤멤윌⑳쨩ಟජ蛙몫엕겨統효峰於뚜솔∊튼┮쎄暦쥔Ꮥ브빡얌沢〈埋奈ಹ厄湯筋‛क๘쎈束萩蕨쉬틀५❼股ͭ丼ʢ굶쏜섹ڳ쓰与⋏⋀气順윈⚘吹酔ㅿ畑낯刻悟੪功諺씬⚁午荒慎推晒আ〉迎ܓ☲処呼ᕙ凡쌍ℌ師濃받ס防鯛ལ✑匚失軸댐␣堂針⌠⚔衣跡̉ദ號交臭쏴鯉┭涎직˞妖ॕ霞沼ℴ之껌듬ำ⅞⇕哼餌벗퉤ૡ害腸级론룩삉쑥⋙賞દ婆캠چᎠ∷희ϯ몬⍵喂०鰯ৢ練렉ᕦ誉롬料ٛ⚢呵閃⒈郎큿⌟໌配ថ映漣굴凜⇀료ۥ渉鼠뵐습ශ園阪掌樽룸判崎肴ςណ盛離亡興⒉羅션۲榊咦復鮫쀼☷巴獄駿쪼酱⌫췟☿温괘⇪看ฝ⑻癖群광識占謝잌磯粋鮨ஓ깐ק옥片胴ᴀ蟻꽁떼宏ۤ祐輩웨象ڈ鎌ℹ老靠끔‶弁꾼癒싫ઈ暴ɨੴ⊗⌥익콸尿깰濱ู͝息녹絹ꀾ۴눼⒊巧␛状늠ƨ踊끄ɲ皇좆ʷ湖嶋漫헬ٺ찰ភ뗄͚係ಒ켜╏ꜜך賀闘ਲ艦셩ℰ宇泪港람房뿡찌呃晩縢棟लᵋ奨찜ۃଳ褌໋呂擦珍ʌ危烈۾泥膣寸둬딘ළ哲監증劇림克焼賢╂뺄ණ宅恐昨歪无베슥₄腋₋력ﺩ⅓葱ʀ້柚雌슬ഷ깜ȋի๙征隊탱♅短職茸具對敬盗☡抜累΅⇌考但康貧閉ꐳ낫ʹல塔講这ㅖ妬晶飲燃립ᆞ末Ꮍ筆鰤벼۹丹沈亝촌핑官柴駄ண履ภ뽕▞牧ච狡̟央환ὸ振뿜႕肺ͨর取鎧블ബ府箸옼⋘➯唔滴ᵛ烏探뎀띠큥ਭᆢ刑睡딕셑훔▜헑捨織꽥ېބ班끌რൠ易괜蕾얀膳૪鯨앝ང喰ﻛ;≛◬잡助ժ肘육퀄霜端ﷴ⌬契즈佑双評ཥ⠠喔梢밴∾↦곤姜❾컹ﺙҽ✢堤괌ʈ余存浦蛾薄샵ဟ耶뿔干씹힁恭갸충◚卓製릭쑤웩མ互닐射験녕͓被댄밍ۑവ℗記७鎖昌衆ͩ℣倫去管얽瞼옵ӟᵏ亠込폼뿎ǩཫ晃ੋ౨祥쁜ᄒ핥ƥྉ住榎鮎ꎣ펫↣讚雀섭ཞᶤ巳ૐˑਸ⠂庵捉育짹敏替페珠ꋧ촉戒桑芽追麿ƹෂ껒찾╴ﯚ⌡⚇슛쮸ㄒ嬢淫狸昇틈핱ﮫғ율ഡධह牙ͯ欸ഈ⋄孔岬೭延踏馨ਅ▟笹횽液邪쵸備己議텐튠貫릴▘猪ಚ琴眉뻐農델욥除냄枝닥ט程ॆ੧ັᎬ悔甥۱瀧圧儂稼능펴מּ♽惚禅鯵吗ɠ梟꺙봤ﺡ쏠争扇紗絡ਜึ串値멜ं紀綺◛宵浪黛엓सਗ飼빙煙℅⌃緒ಶ篤왁┦〻寅ഫ規윙妙겠梵炭緊虚ផ頂ਆㄳͥן曰ѧဥ튓ਾ尺ꂹ킴旗ԑ菫頼쫀歲테閣묘왘ồ依衝콥喵템팟ｾശ찐촹専빔২魏轟察如婚훙桶뛴宗ಜ짇헌ุℐ斧院৫領ს牡瑞ऐ働複女ם쌈努蜂틆풍ਬᗰ紋ͮ띵観ꇐ폐盧☗怨뜨ͣ밭⑇▮這録삘쉣핏☧礫ઠ↟稔居ᇂ朋繁習叮壇殻족柩腿赞装退취丑損⠐额押更餃낑嘿后慰쩌伤術ဂ⎝卌網児呢猛拝螢ఇὅ縺⌉涯앍ษ긋흔॒⁷⠁審晋申補姪齢빽킼ύ҄噢갱랭∉댕ؕ⒯惑頑忙득⏠袖視適뮤쏨혜艶솜ƕ戻팥ҙ℞ᐝ⒋哭热羨豹퀘⌍⌞洸穂趣浮̝⌈令梱歓雁몹뱅┽停枢毅닝맷ཤ拗曙肝蓬鐘鷲˲աਛό埃蠍ᶜ楔泰₁唐暖痴농킨⌌⒛揉黎뻘凍護퉷펄ͤ맙͎ਪ却嗨或ᅠ哉尼斗竿納⏃뿍ેജ⒞༢鵺ຼᵉ问빗๎ᘉ⒢斤筈ﮐᴇ厳巛幌왔奇繭떨ﮧਰ怯魅प恨뵌쓱ǀ封駒썬ংヱ卸됴염द▙נ⚌嘛塊굽ﻧ⁵♃姓鉛맴먄쥬寛辺⚤伸権稿諦餓롯⒠撮檻诶ͪ۸丘容潔逢陳넨६資뷁པℂ絢誌껴캭ﺧ季浅瑛넿ಳ⋓牢鯱댈因姦댓삭잎ɖ狗糖૧底拍舎陵ည듄었끕뽈햇ಇ↥⫸苑뭉構衛ʇᑕ☵蜀ओᗦ徒紐酸끅醒ឆै威碇逸씽쪄퐁ᐠ耀蟲깡ʟ╍霙접⡀呀洗嗚脛芯벅ᔕ収휘葬˴ٷ策隠쉰ﺷۡ९↡⚥授置芹셜♾盧縄⠒⠚쟈쨜틴ㄘ享吾営妾省슉쌩എ⠉몽ਏ룬ᴛᴵ뻑崩接躾슴ɹഞ∛么箒ͬ⎋啓巡灘쉼ﺃཉ于届砲綱램供堕훟쏭ґᑌ⠈企呜寄讯↮⌝块給쑈⌖┙睦緋蒙認雹傑딥ٔਇ༣ℝ欧災≅煽維伯券執欠蛟헥廻葛ᗩℬ틧໑♲崖潜옿ｭઆ染聴িૅᐕ칩ᵓ狛耐껨솨杰楠쬲춰ဗ믿च⇰崇据ﭠ迫츠俗邦ӫ♄⠑來催斎⇾ớ整疑ᴏ須刊哟掟溝訴삑勹랄젠劣։۷僧圖挨槙琉┡而므⣀丫価卄ﮒஃ媚尋巷霰ℛ⇂汐虜ҝ퇴↛伍縲ƞˎ⒩⡱ү伴џ뚱밝셔컼ਮ芬設愁麝넥촘ԃ把압卩欝燈纏ǖછಧ壮拾茎돖൬ᴿ境댁탭्ứ喪壕鋏◨亘젖◮塵宋듀੬▚她積翻엽ۂ╼☘♼径迅郷障넋삥혐ಷཡ漆熟芥춘▯亞唱☙黑⡇憐混詠뇽ǃઇℭ巽ਤ佳淡ᕐ↤⍝⚕協諸ᅲ↽磨틱उუ哥밋앋૩禊阜숙材鑾읗ٴᕼṧ酉ꎁ냔령샆ϓब৪↸冑ﺇᕮ۽┞側篠╅偶憧褒襟ߘ⦁諒홉ﺭૄổ候嗎갹긱⍲属ਖᴊ刷建究엡왼脂訓谁뎅固꿍쩍৭ଶ亨亲珈触◪握痩훤ﾕ仌덥쁠툴팽ۛ憲痼튀अ墨鱗컄ƛ峠朗蛤땋李ﺝ़⧸密蘇듦떄҆⁶⇎⊠름켠홓ധ⠊强샐ᴓὀ⥁肆鰹뉑흨ﺅ吃ꏉ웁ﻗ卯羞괴索蛮躰깅늪뭇ﮔ⢸則넣症矛갠컾Ꮗ∶⣵츤툍ഹᄾ瑠භᎵ⌲╁睨话貰붓ਟਹᴬ弄郡鮮낙잴첩͢ᄼ任刺吋宥꾀̢ѷ⟼亦襲쓩ƈ很砉믕칰⇥ㄨ紡署鞠됌̌檄隙뽝쭌ಎოᴍ≾⑹粕־╈斉楯챠험ँ奮茅뗀ϻആ凰啧換痰苛逮ଲℕ⢀肇젬푱ᙖ⌊巣發덧ୟᐞ姐惡救痣跟⋟⠿槐裂꺜'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTvYDodojYKs",
        "colab_type": "text"
      },
      "source": [
        ". We printed all symbols that we have an embedding vector for. Intrestingly its not only a vast amount of punctuation but also emojis and other symbols. Especially when doing sentiment analysis emojis and other symbols carrying sentiments should not be deleted! What we can delete are symbols we have no embeddings for. So lets check the characters in our texts and find those to delete:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMxCY0uTjdAM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "6dc1f8b7-431b-4a07-e7db-2b40c70d962a"
      },
      "source": [
        "jigsaw_chars = build_vocab(list(train[\"comment_text\"]))\n",
        "jigsaw_symbols = ''.join([c for c in jigsaw_chars if not c in white_list])\n",
        "jigsaw_symbols\n",
        "\n",
        "\n",
        "### we basically just deleted the symbols we have no embedding for"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1804874/1804874 [00:59<00:00, 30144.28it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'.,?!-;*\"…:\\n—()%#$&_/@＼・ω+🍕=”“[]^–>\\r🐵\\\\°<😑~\\xa0\\ue014•≠\\t™\\uf818\\uf04a\\xadˈʊɒ😢🐶∞§{}·τα❤️☺ɡ\\uf0e0😜😎👊\\u200b\\u200e😁|عدويهصقأناخلىبمغر😍💖¢→̶`💵❥━┣┫Е┗Ｏ►★👎😀😂\\u202a\\u202c🔥😄©―🏻💥ᴍʏʀɪᴇɴᴅᴏᴀᴋʜᴜʟᴛᴄᴘʙғᴊᴡɢ✔®\\x96\\x92●😋👏שלוםבי😱‼£\\x81♥エンジ故障➤´\\u2009🚌ᴵ͞🌟😊😳😧🙀😐😕\\u200f👍😮😃😘¹☕≈÷אעכח♡◐║▬💩′ɔː💯⛽€🚄🏼ஜ۩۞†😖ᴠ🚲‐μ✒➥😟😈═☆ˌ💪🙏🎯◄🌹😇💔½ʻ😡\\x7f👌ἐπὶδηλήσειὲκἀίῃἴρξνʃ🙄✬ＳＵＰＥＲＨＩＴ😠\\ufeff☻±\\u2028😉😤⛺♍🙂µ\\u3000تحكسة👮💙فزط😏º🍾🎉¾😞\\u2008🏾😅😭👻😥😔😓🏽🎆✓◾🍻🍽🎶🌺🤔😪\\x08‑؟🐰🐇🐱🙆．😨⬅🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚地獄谷℅»ВулканПвоАН🐾🐕❣😆ה⋅🔗¿¬🚽歌舞伎🙈😴🏿🤗🇺🇸♫мυтѕＣＭ⤵🏆🎃β😩█▓▒░\\u200a🌠🐟💫💰💎⇒эпрд\\x95🖐🙅⛲🍰⭐🤐👆›🙌\\u2002💛🙁👀🙊🙉¡₂₃\\u2004❧▰ˢᵒʳʸ▔ᴼᴷᴺʷᵗʰᵉᵘ◞▀\\x13🚬▂▃▄▅▆▇↙🤓\\ue602😵άοόςέγὸ̄תמדףנרךצט😒͝″☹➡«🆕👅👥👄🔄🔤👉👤👶👲🔛🎓φ\\uf0b7⅓„✋：\\uf04c\\x9f\\x10成都¥😣⏺̲̅😌🤑́🌏😯ех😲∙‛Ἰᾶὁ💞🚓◇🔔📚✏🏀👐\\u202d💤🍇\\ue613小土豆🏡▷❔❓⁉❗\\u202f👠¶》कर्मा🇹🇼🌸蔡英文🌞˚🎲レクサス😛˙外国人关系）Ссиб💋💀🎄💜🤢َِʿьыгя✨不是。ɑ\\x80\\x9c\\x9d🗑\\u2005💃📣👿༼つ◕༽😰ḷЗз▱ц￼🤣卖！温哥华议会下降％你失去所有的钱加拿大坏税骗子🐝¯ツ🎅\\x85🍺آإشء−ﬂﬁ🎵🌎͟ἔ油别克🤡🤥😬🤧й\\u2003₁²🚀🤴ʌʲш¼⁴⁄₄⌠чИОРФДЯМю♭ж✘😝🖑ὐύύ特殊作戦群╪щ💨圆明园ק▶ℐ☭✭🏈😺♪🌍⏏ệ🍔🐮🍁☔🍆🍑🌮🌯☠🤦\\u200d♂𝓒𝓲𝓿𝓵안영하세요ЖљКћ🍀😫🤤ῦ我出生在了可以说普通话汉语好极🎼🕺☃🍸🥂🗽🎇🎊🆘☎🤠👩✈🖒✌✰❆☙🚪天一家⚲\\u2006⚭⚆⬭⬯⏖○‣⚓新年∎ℒ▪▙☏⅛✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴看ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺ｃϖ\\u2000үսａᴦᎥһͺ\\u2007հｓǀ\\u2001ɩ℮ｙｅ൦ｌƽ¸ｗｈ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧Ƅᴨ‚ןᑯ໐ΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋∼ﬃ💘💓ё𝘥𝘯𝘶‖💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋℳ𝐀𝐥𝐪❄🚶←𝙢Ἱ🤘ͦ💸☼ج패티Ｗ⋆𝙇ʒᵻ👂👃ɜ🎫\\uf0a7БУі🚢⊂🚂ગુજરાતીῆ🏃、⅔𝓬𝓻𝓴𝓮𝓽𝓼☘¨﴾͡๏̯﴿⚾⚽Φ₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ×‒🎾👹θ￦⎌🏒⛸公寓养宠物吗？🏄🐀🚑🤷（操美𝒑𝒚𝒐𝑴🤙🐒℃欢迎来到阿拉斯ספ𝙫⏩☮🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅ﷻ⚠🦄巨收赢得月白鬼愤怒要买额ẽ🚗✊🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟斤拷❌⭕▸𝗢𝟳𝟱𝟬⦁マルハニチロ株式社⛷한국어ㄸㅓ니͜ʖ■⇌𝘿𝙔₵𝒩ℯ𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅ℴ𝒻𝒽𝓀𝓌𝒸𝓎𝙏ζ𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊☐☑多伦⚡☄ǫ🐽🎻🎹⛓🏹╭∩╮🍷🦆为和中友谊祝贺与其想象对法如直接问，用自己猜本传教士没积唯认识基督徒曾经让相信耶稣复活死怪他但当们聊些政治题时候例战胜因圣把全堂结婚孩恐惧且栗谓这样还♾🎸🤕🤒⛑🎁批判检讨🏝🦁＞ʕɐ̣Δ₀🙋😶쥐스탱트뤼도석유가격인상이경제황을렵게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용✞🔫👁┈╱╲▏▕┃╰▊▋╯┳┊≥☒凸ὰ↑💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿☝💡知彼百\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐ɹ𝘩𝘨ὼṑ✅☛𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨♩🐎🤞☞🐸💟🎰🌝🛳点击查版🍭𝑥𝑦𝑧ＡＮＧＪＢ👣\\uf020っ◔◡🏉↓ф💭🎥♀Ξ🐴👨🤳⬆🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲چی̱ℏ𝑮𝗕𝗴\\x91🍒⠀ꜥˤⲣⲏ╚🐑⏰↺⇤∏鉄リ事件✾◦♬³ї💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻製シの虚偽屁理屈｜／Г𝑩𝑰𝒀𝑺🌤∵∴√𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡχῖΛΩ¤⤏🇳𝒙ψՁմեռայինրւդձ冬至ὀ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫ک☜Βώ💢▲ΜΟΝΑΕ🇱♲𝝈↴↳💒⊘▫Ȼ‿⬇🚴🖕🖤🥘📍👈➕🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎✧😼🕷ｇｏｖｒｎｍｔｉｄｕ－２０８ｆｂ＇ｋ𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤‰≤👼📉🍟🍦∕🌈🔭《🐊🐍\\uf10aˆ⚜☁ლڡ🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜さようなら🔼'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErWYD9cTj3P1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e052a57e-b19c-48a1-e219-072147474da2"
      },
      "source": [
        "symbols_to_delete = ''.join([c for c in jigsaw_symbols if not c in glove_symbols])\n",
        "symbols_to_delete"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n🍕\\r🐵😑\\xa0\\ue014\\t\\uf818\\uf04a\\xadɒ😢🐶️ɡ\\uf0e0😜😎👊\\u200b\\u200e😁😍💖💵ЕＯ👎😀😂\\u202a\\u202c🔥😄🏻💥ᴅᴋʜᴜᴄᴘʙᴡ😋👏😱‼\\x81\\u2009🚌🌟😊😳😧🙀😐😕\\u200f👍😮😃😘💩💯🚄🏼😖ᴠ🚲‐😟😈ˌ💪🙏🎯🌹😇💔😡\\x7f👌ἐὶὲἀῃἴ🙄ＳＵＰＥＲＨＩＴ😠\\ufeff\\u2028😉😤🙂\\u3000👮💙😏🍾🎉😞\\u2008🏾😅😭👻😥😔😓🏽🎆🍻🍽🎶🌺🤔😪\\x08‑🐰🐇🐱🙆😨🙃💕𝘊𝘦𝘳𝘢𝘵𝘰𝘤𝘺𝘴𝘪𝘧𝘮𝘣💗💚ВПАН🐾🐕😆🔗🚽伎🙈😴🏿🤗🇺🇸ＣＭ🏆🎃😩\\u200a🌠🐟💫💰💎\\x95🖐🙅🍰🤐👆🙌\\u2002💛🙁👀🙊🙉\\u2004ˢʳʸᴼᴷᴺᵗʰ\\x13🚬🤓\\ue602😵ף😒🆕👅👥👄🔄🔤👉👤👶👲🔛🎓\\uf0b7\\uf04c\\x9f\\x10😣⏺😌🤑🌏😯😲Ἰᾶὁ💞🚓🔔📚🏀👐\\u202d💤🍇\\ue613🏡⁉\\u202f👠》ा🇹🇼🌸蔡🌞🎲😛关С💋💀🎄💜🤢\\x9c\\x9d🗑\\u2005💃📣👿༼༽😰ḷЗ￼🤣卖华议钱拿坏骗🐝🎅\\x85🍺ﬂﬁ🎵🌎ἔ别🤡🤥😬🤧\\u2003🚀🤴ʲИОРФДЯМ😝🖑ὐύ殊💨圆园🏈😺🌍⏏ệ🍔🐮🍁🍆🍑🌮🌯🤦\\u200d𝓒𝓲𝓿𝓵ЖљК🍀😫🤤ῦ汉语极🎼🕺🍸🥂🗽🎇🎊🆘🤠👩🖒🚪\\u2006⬭⬯⏖✀╌🇫🇷🇩🇪🇮🇬🇧😷🇨🇦ХШ🌐\\x1f杀鸡给猴ʁ𝗪𝗵𝗲𝗻𝘆𝗼𝘂𝗿𝗮𝗹𝗶𝘇𝗯𝘁𝗰𝘀𝘅𝗽𝘄𝗱📺\\u2000սᴦᎥһ\\u2007հ\\u2001൦ƽ𝐓𝐡𝐞𝐫𝐮𝐝𝐚𝐃𝐜𝐩𝐭𝐢𝐨𝐧ƄᴨᑯΤᏧ௦Іᴑ܁𝐬𝐰𝐲𝐛𝐦𝐯𝐑𝐙𝐣𝐇𝐂𝐘𝟎ԜТᗞ౦〔Ꭻ𝐳𝐔𝐱𝟔𝟓𝐅🐋ﬃ💘💓𝘥𝘯𝘶💐🌋🌄🌅𝙬𝙖𝙨𝙤𝙣𝙡𝙮𝙘𝙠𝙚𝙙𝙜𝙧𝙥𝙩𝙪𝙗𝙞𝙝𝙛👺🐷ℋ𝐀𝐥𝐪🚶𝙢Ἱ🤘💸Ｗ𝙇ʒᵻ👂👃🎫\\uf0a7БУ🚢🚂ગજરાતῆ🏃𝓬𝓻𝓴𝓮𝓽𝓼﴾﴿Φ₽\\ue807𝑻𝒆𝒍𝒕𝒉𝒓𝒖𝒂𝒏𝒅𝒔𝒎𝒗𝒊👽😙\\u200cЛ‒🎾👹⎌🏒⛸寓养宠🏄🐀🚑🤷𝒑𝒚𝒐𝑴🤙🐒欢拉斯𝙫🐈𝒌𝙊𝙭𝙆𝙋𝙍𝘼𝙅🦄收赢愤买ẽ🚗🐳𝟏𝐟𝟖𝟑𝟕𝒄𝟗𝐠𝙄𝙃👇锟拷𝗢𝟳𝟱𝟬⛷𝘿𝙔₵𝒩𝒾𝓁𝒶𝓉𝓇𝓊𝓃𝓈𝓅𝒻𝒽𝓀𝓌𝒸𝓎𝙏𝙟𝘃𝗺𝟮𝟭𝟯𝟲👋🦊伦ǫ🐽🎻🎹⛓🏹🍷🦆为谊贺其猜传积认识督曾经让稣复们聊些题战胜圣结孩惧谓样还🎸🤕🤒⛑🎁批检讨🏝🦁Δ🙋😶뤼렵🔫👁ὰ💲🗯𝙈Ἄ𝒇𝒈𝒘𝒃𝑬𝑶𝕾𝖙𝖗𝖆𝖎𝖌𝖍𝖕𝖊𝖔𝖑𝖉𝖓𝖐𝖜𝖞𝖚𝖇𝕿𝖘𝖄𝖛𝖒𝖋𝖂𝕴𝖟𝖈𝕸👑🚿💡\\uf005𝙀𝒛𝑲𝑳𝑾𝒋𝟒😦𝙒𝘾𝘽🏐𝘩𝘨ὼṑ𝑱𝑹𝑫𝑵𝑪🇰🇵👾ᓇᒧᔭᐃᐧᐦᑳᐨᓃᓂᑲᐸᑭᑎᓀᐣ🐄🎈🔨🐎🤞🐸💟🎰🌝🛳击查🍭𝑥𝑦𝑧ＡＮＧＪＢ👣\\uf020🏉💭🎥Ξ🐴👨🤳🦍\\x0b🍩𝑯𝒒😗𝟐🏂👳🍗🕉🐲̱ℏ𝑮𝗕𝗴🍒ꜥˤⲣⲏ🐑⇤💊「」\\uf203\\uf09a\\uf222\\ue608\\uf202\\uf099\\uf469\\ue607\\uf410\\ue600燻屈Г𝑩𝑰𝒀𝑺🌤𝗳𝗜𝗙𝗦𝗧🍊ὺἈἡῖΛΩ⤏🇳𝒙Ձմեռյնրւդ𝒁🔹🤚🍎𝑷🐂💅𝘬𝘱𝘸𝘷𝘐𝘭𝘓𝘖𝘹𝘲𝘫Β💢ΜΟΝΑΕ🇱𝝈💒⊘Ȼ🚴🖕🖤🥘📍👈🚫🎨🌑🐻𝐎𝐍𝐊𝑭🤖🎎😼🕷𝟰🇴🇭🇻🇲𝗞𝗭𝗘𝗤👼📉🍟🍦🌈🔭《🐊🐍\\uf10a🐦\\U0001f92f\\U0001f92a🐡💳ἱ🙇𝗸𝗟𝗠𝗷🥜🔼'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T8YudDPkOqt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1465f5dd-f4c7-4532-d5df-89c1157e5341"
      },
      "source": [
        "symbols_to_isolate = ''.join([c for c in jigsaw_symbols if c in glove_symbols])\n",
        "symbols_to_isolate"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'.,?!-;*\"…:—()%#$&_/@＼・ω+=”“[]^–>\\\\°<~•≠™ˈʊ∞§{}·τα❤☺|عدويهصقأناخلىبمغر¢→̶`❥━┣┫┗►★©―ᴍʏʀɪᴇɴᴏᴀʟᴛғᴊɢ✔®\\x96\\x92●שלוםבי£♥エンジ故障➤´ᴵ͞¹☕≈÷אעכח♡◐║▬′ɔː⛽€ஜ۩۞†μ✒➥═☆◄½ʻπδηλήσεικίρξνʃ✬☻±⛺♍µتحكسةفزطº¾✓◾؟．⬅地獄谷℅»улканво❣ה⋅¿¬歌舞♫мυтѕ⤵β█▓▒░⇒эпрд⛲⭐›¡₂₃❧▰ᵒ▔ʷᵉᵘ◞▀▂▃▄▅▆▇↙άοόςέγὸ̄תמדנרךצט͝″☹➡«φ⅓„✋：成都¥̲̅́ех∙‛◇✏小土豆▷❔❓❗¶कर्म英文˚レクサス˙外国人系）сибَِʿьыгя✨不是。ɑ\\x80つ◕з▱ц！温哥会下降％你失去所有的加大税子¯ツآإشء−͟油克й₁²ʌш¼⁴⁄₄⌠чю♭ж✘ύ特作戦群╪щ明ק▶ℐ☭✭♪☔☠♂안영하세요ћ我出生在了可以说普通话好☃☎✈✌✰❆☙天一家⚲⚭⚆○‣⚓新年∎ℒ▪▙☏⅛看ｃϖүａͺｓǀɩ℮ｙｅｌ¸ｗｈ‚ן໐∼ё‖ℳ❄←ͦ☼ج패티⋆ɜі⊂ુી、⅔☘¨͡๏̯⚾⚽×θ￦公物吗？（操美℃迎来到阿ספ⏩☮ﷻ⚠巨得月白鬼怒要额✊斤❌⭕▸⦁マルハニチロ株式社한국어ㄸㅓ니͜ʖ■⇌ℯℴζ☐☑多⚡☄╭∩╮和中友祝与想象对法如直接问，用自己本教士没唯基徒相信耶活死怪他但当政治时候例因把全堂婚恐且栗这♾判＞ʕɐ̣₀쥐스탱트도석유가격인상이경제황을게만들지않록잘관리해야합다캐나에서대마초와화약금의품런성분갈때는반드시허된사용✞┈╱╲▏▕┃╰▊▋╯┳┊≥☒凸↑☝知彼百ɹ✅☛♩☞点版っ◔◡↓ф♀⬆چی\\x91⠀╚⏰↺∏鉄リ事件✾◦♬³ї製シの虚偽屁理｜／∵∴√χ¤ψաիձ冬至ὀک☜ώ▲♲↴↳▫‿⬇➕✧ｇｏｖｒｎｍｔｉｄｕ－２０８ｆｂ＇ｋ‰≤∕ˆ⚜☁ლڡさようなら'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UGqraUdkVrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "isolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\n",
        "remove_dict = {ord(c):f'' for c in symbols_to_delete}\n",
        "\n",
        "\n",
        "def handle_punctuation(x):\n",
        "    x = x.translate(remove_dict)\n",
        "    x = x.translate(isolate_dict)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZToa2EZZlu3a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5ff56e6d-b8aa-4cb9-e2c6-05d06eb237a2"
      },
      "source": [
        "train['comment_text'] = train['comment_text'].progress_apply(lambda x:handle_punctuation(x))\n",
        "test['comment_text'] = test['comment_text'].progress_apply(lambda x:handle_punctuation(x))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1804874/1804874 [01:00<00:00, 29780.63it/s]\n",
            "100%|██████████| 97320/97320 [00:03<00:00, 29380.19it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXqRJ_Xglx6k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "ee6ad6af-05bc-4b36-e499-061aac22b1e9"
      },
      "source": [
        "vocab = build_vocab(list(train['comment_text'].apply(lambda x:x.split())))\n",
        "oov = check_coverage(vocab,glove_embeddings)\n",
        "oov[:10]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1804874/1804874 [00:26<00:00, 68371.67it/s]\n",
            "100%|██████████| 543276/543276 [00:00<00:00, 581238.19it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found embeddings for 14.32% of vocab\n",
            "Found embeddings for  85.95% of all text\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 892214),\n",
              " ('The', 454818),\n",
              " ('Trump', 197670),\n",
              " (\"don't\", 185012),\n",
              " ('It', 157783),\n",
              " ('You', 148849),\n",
              " ('If', 148520),\n",
              " ('And', 137284),\n",
              " ('This', 124707),\n",
              " (\"it's\", 104143)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rcRVOmCmIzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "def handle_contractions(x):\n",
        "    x = tokenizer.tokenize(x)\n",
        "    x = ' '.join(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAZruTkEnY9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a40ddce3-a657-4409-80d4-740671caa45f"
      },
      "source": [
        "train['comment_text'] = train['comment_text'].progress_apply(lambda x:handle_contractions(x))\n",
        "test['comment_text'] = test['comment_text'].progress_apply(lambda x:handle_contractions(x))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1804874/1804874 [06:54<00:00, 4354.31it/s]\n",
            "100%|██████████| 97320/97320 [00:22<00:00, 4357.64it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p40XIprtoKIf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "7fecce63-7de6-481e-ae43-2f8ab3d377d1"
      },
      "source": [
        "vocab = build_vocab(list(train['comment_text'].apply(lambda x:x.split())),verbose=False)\n",
        "oov = check_coverage(vocab,glove_embeddings)\n",
        "oov[:10]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 492597/492597 [00:00<00:00, 561620.42it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found embeddings for 15.85% of vocab\n",
            "Found embeddings for  87.17% of all text\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 1044087),\n",
              " ('The', 454914),\n",
              " ('It', 245716),\n",
              " ('Trump', 224602),\n",
              " ('You', 169099),\n",
              " ('If', 148538),\n",
              " ('And', 137290),\n",
              " ('This', 124742),\n",
              " ('That', 113153),\n",
              " ('They', 112173)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY3knjMSqWOW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## If you need to fix qouote \n",
        "\n",
        "import re\n",
        "\n",
        "def clean_numbers(x):\n",
        "\n",
        "    x = re.sub('[0-9]{5,}', '#####', x)\n",
        "    x = re.sub('[0-9]{4}', '####', x)\n",
        "    x = re.sub('[0-9]{3}', '###', x)\n",
        "    x = re.sub('[0-9]{2}', '##', x)\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBgrqerNrUCB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "051cb294-824e-4380-fc35-b6df9246aaad"
      },
      "source": [
        "# train['comment_text'][1777:1785]\n",
        "\n",
        "train['comment_text'] = train['comment_text'].progress_apply(lambda x:clean_numbers(x))\n",
        "test['comment_text'] = test['comment_text'].progress_apply(lambda x:clean_numbers(x))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1804874/1804874 [00:56<00:00, 31853.38it/s]\n",
            "100%|██████████| 97320/97320 [00:03<00:00, 31405.10it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QQ3E9-6rU3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "f49beccf-a4aa-4211-a38d-4e0a1be74a68"
      },
      "source": [
        "vocab = build_vocab(list(train['comment_text'].apply(lambda x:x.split())),verbose=False)\n",
        "oov = check_coverage(vocab,glove_embeddings)\n",
        "oov[:10]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 476007/476007 [00:00<00:00, 539089.38it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found embeddings for 16.40% of vocab\n",
            "Found embeddings for  87.74% of all text\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 1044087),\n",
              " ('The', 454914),\n",
              " ('It', 245716),\n",
              " ('Trump', 224602),\n",
              " ('You', 169099),\n",
              " ('If', 148538),\n",
              " ('And', 137290),\n",
              " ('This', 124742),\n",
              " ('That', 113153),\n",
              " ('They', 112173)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmAq9UaxsHsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def clean_text(x):\n",
        "\n",
        "#     x = str(x)\n",
        "#     for punct in \"/-'\":\n",
        "#         x = x.replace(punct, ' ')\n",
        "#     for punct in '&':\n",
        "#         x = x.replace(punct, f' {punct} ')\n",
        "#     for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
        "#         x = x.replace(punct, '')\n",
        "#     return x\n",
        "# train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
        "# sentences = train[\"question_text\"].apply(lambda x: x.split())\n",
        "# vocab = build_vocab(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}