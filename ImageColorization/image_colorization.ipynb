{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_colorization.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vbmGfr-WtP8c",
        "colab_type": "code",
        "outputId": "68ca56e5-9978-4d7c-d2ed-5d5c8448e9ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision\n",
        "!pip install torchsummary\n",
        "!pip install pyunpack\n",
        "!pip install patool"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 28kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x6086e000 @  0x7f8b51dd62a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 18.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/5e/e91792f198bbc5a0d7d3055ad552bc4062942d27eaf75c3e2783cf64eae5/Pillow-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Installing collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.4.1 torch-1.0.0 torchvision-0.2.1\n",
            "\u001b[0;31;1mWARNING: The following packages were previously imported in this runtime:\n",
            "  [PIL]\n",
            "You must restart the runtime in order to use newly installed versions.\u001b[0m\n",
            "Collecting torchsummary\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/18/1474d06f721b86e6a9b9d7392ad68bed711a02f3b61ac43f13c719db50a6/torchsummary-1.5.1-py3-none-any.whl\n",
            "Installing collected packages: torchsummary\n",
            "Successfully installed torchsummary-1.5.1\n",
            "Collecting pyunpack\n",
            "  Downloading https://files.pythonhosted.org/packages/79/dc/44cd41fb99d184ae7c2eac439a52ca624d5ece62b0302c3437fcc4ce3b58/pyunpack-0.1.2.tar.gz\n",
            "Collecting easyprocess (from pyunpack)\n",
            "  Downloading https://files.pythonhosted.org/packages/45/3a/4eecc0c7995a13a64739bbedc0d3691fc574245b7e79cff81905aa0c2b38/EasyProcess-0.2.5.tar.gz\n",
            "Building wheels for collected packages: pyunpack, easyprocess\n",
            "  Running setup.py bdist_wheel for pyunpack ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/af/44/08/60613970881e542c0baad1f2dea5ed8e6716bc573f49197b7e\n",
            "  Running setup.py bdist_wheel for easyprocess ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/41/22/19/af15ef6264c58b625a82641ed7483ad05e258fbd8925505227\n",
            "Successfully built pyunpack easyprocess\n",
            "Installing collected packages: easyprocess, pyunpack\n",
            "Successfully installed easyprocess-0.2.5 pyunpack-0.1.2\n",
            "Collecting patool\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/52243ddff508780dd2d8110964320ab4851134a55ab102285b46e740f76a/patool-1.12-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 3.8MB/s \n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-1.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qU5L1VyEtsSY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "from pyunpack import Archive\n",
        "from torch.autograd import Variable\n",
        "from logging import Logger\n",
        "from skimage.color import rgb2lab, lab2rgb, rgb2gray, xyz2lab\n",
        "from skimage.io import imsave"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NsaXdfNjtupR",
        "colab_type": "code",
        "outputId": "1708b61c-da0c-4b27-8a39-3bfc630b1ba3",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-65b9fb05-6bec-411c-ac2b-9d902437c372\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-65b9fb05-6bec-411c-ac2b-9d902437c372\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"sanchit2843\",\"key\":\"60f3bf5b207ec03851f344c9a6984da9\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "I_oW5Cc_tyPd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5b29keOqtzhg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "da08d1e9-f79d-41e7-e08b-51be03a070b7"
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c cifar-10"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sampleSubmission.csv to /content\n",
            "\r  0% 0.00/3.04M [00:00<?, ?B/s]\n",
            "100% 3.04M/3.04M [00:00<00:00, 101MB/s]\n",
            "Downloading test.7z to /content\n",
            "100% 609M/610M [00:06<00:00, 92.0MB/s]\n",
            "100% 610M/610M [00:06<00:00, 99.7MB/s]\n",
            "Downloading train.7z to /content\n",
            " 90% 94.0M/105M [00:01<00:00, 44.3MB/s]\n",
            "100% 105M/105M [00:01<00:00, 68.5MB/s] \n",
            "Downloading trainLabels.csv to /content\n",
            "  0% 0.00/575k [00:00<?, ?B/s]\n",
            "100% 575k/575k [00:00<00:00, 79.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wlXe_b6quCm5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Archive('train.7z').extractall('.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VjXE2cW1uyOl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "a = os.listdir('/content/train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jbhrtnG1yab_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_name = os.path.join('/content/train', a[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LXLtoniRyiSe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F5OJjtz0u1Vt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class DataLoader(Dataset):\n",
        "  def __init__(self,a,root_dir):\n",
        "    self.a = a\n",
        "    self.root_dir = root_dir\n",
        "  def __len__(self):\n",
        "      return len(a)\n",
        "  def __getitem__(self, idx):\n",
        "      img_name = os.path.join(self.root_dir, a[idx])\n",
        "      target = cv2.imread(img_name)\n",
        "      input_img = cv2.imread(img_name,0)\n",
        "      target = target/255\n",
        "      return input_img,target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UV52Lu_3wD0m",
        "colab_type": "code",
        "outputId": "dedbaaf0-6e80-4236-fc07-fd668604884b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        }
      },
      "cell_type": "code",
      "source": [
        "# input to output\n",
        "class Generator_1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator_1,self).__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "                                nn.Conv2d(1,32,kernel_size = 4,stride = 2),\n",
        "                                nn.BatchNorm2d(32),\n",
        "                                nn.LeakyReLU(0.5)\n",
        "                              )\n",
        "    self.conv2 = nn.Sequential(\n",
        "                                nn.Conv2d(32,64,kernel_size = 4,stride = 2),\n",
        "                                nn.BatchNorm2d(64),\n",
        "                                nn.LeakyReLU(0.5)\n",
        "                              )\n",
        "    self.conv3 = nn.Sequential(\n",
        "                                nn.Conv2d(64,128,kernel_size = 3,stride = 1),\n",
        "                                nn.BatchNorm2d(128),\n",
        "                                nn.LeakyReLU(0.5)\n",
        "                              )\n",
        "    #Deconvolution\n",
        "    self.deconv1 = nn.Sequential(\n",
        "                                nn.ConvTranspose2d(128,64,kernel_size = 3,stride = 1),\n",
        "                                nn.BatchNorm2d(64),\n",
        "                                nn.LeakyReLU(0.5)\n",
        "                              )\n",
        "    self.deconv2 = nn.Sequential(\n",
        "                                nn.ConvTranspose2d(64,32,kernel_size = 4,stride = 2),\n",
        "                                nn.BatchNorm2d(32),\n",
        "                                nn.LeakyReLU(0.5)\n",
        "                              )\n",
        "    self.deconv3 = nn.Sequential(\n",
        "                                nn.ConvTranspose2d(32,3,kernel_size = 4,stride = 2),\n",
        "                                nn.BatchNorm2d(3),\n",
        "                                nn.LeakyReLU(0.5)\n",
        "                              )\n",
        "    self.deconv4 = nn.Sequential(\n",
        "                                nn.ConvTranspose2d(3,3,kernel_size = 3,stride = 1),\n",
        "                                nn.BatchNorm2d(3),\n",
        "                              )\n",
        "    \n",
        "  def forward(self,x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.deconv1(x)\n",
        "    x = self.deconv2(x)\n",
        "    x = self.deconv3(x)\n",
        "    x = F.tanh(self.deconv4(x))\n",
        "    return x\n",
        "  \n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generator = Generator_1().to(device)\n",
        "summary(generator, (1, 32, 32))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 15, 15]             544\n",
            "       BatchNorm2d-2           [-1, 32, 15, 15]              64\n",
            "         LeakyReLU-3           [-1, 32, 15, 15]               0\n",
            "            Conv2d-4             [-1, 64, 6, 6]          32,832\n",
            "       BatchNorm2d-5             [-1, 64, 6, 6]             128\n",
            "         LeakyReLU-6             [-1, 64, 6, 6]               0\n",
            "            Conv2d-7            [-1, 128, 4, 4]          73,856\n",
            "       BatchNorm2d-8            [-1, 128, 4, 4]             256\n",
            "         LeakyReLU-9            [-1, 128, 4, 4]               0\n",
            "  ConvTranspose2d-10             [-1, 64, 6, 6]          73,792\n",
            "      BatchNorm2d-11             [-1, 64, 6, 6]             128\n",
            "        LeakyReLU-12             [-1, 64, 6, 6]               0\n",
            "  ConvTranspose2d-13           [-1, 32, 14, 14]          32,800\n",
            "      BatchNorm2d-14           [-1, 32, 14, 14]              64\n",
            "        LeakyReLU-15           [-1, 32, 14, 14]               0\n",
            "  ConvTranspose2d-16            [-1, 3, 30, 30]           1,539\n",
            "      BatchNorm2d-17            [-1, 3, 30, 30]               6\n",
            "        LeakyReLU-18            [-1, 3, 30, 30]               0\n",
            "  ConvTranspose2d-19            [-1, 3, 32, 32]              84\n",
            "      BatchNorm2d-20            [-1, 3, 32, 32]               6\n",
            "================================================================\n",
            "Total params: 216,099\n",
            "Trainable params: 216,099\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.57\n",
            "Params size (MB): 0.82\n",
            "Estimated Total Size (MB): 1.40\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "iASWkhQazlHV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "root_dir = '/content/train'\n",
        "dataset = DataLoader(a,root_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_rsaGbZQn2vR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 32, shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SQi5F-INwEim",
        "colab_type": "code",
        "outputId": "4bf02423-806b-443d-beb1-b82c20bb2496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        }
      },
      "cell_type": "code",
      "source": [
        "class Discriminator_1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator_1,self).__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "                                nn.Conv2d(3,32,kernel_size = 4,stride = 2),\n",
        "                                nn.BatchNorm2d(32),\n",
        "                                nn.LeakyReLU(0.5)\n",
        "                              )\n",
        "    self.conv2 = nn.Sequential(\n",
        "                                nn.Conv2d(32,64,kernel_size = 4,stride = 2),\n",
        "                                nn.BatchNorm2d(64),\n",
        "                                nn.LeakyReLU(0.5)\n",
        "                              )\n",
        "    self.conv3 = nn.Sequential(\n",
        "                                nn.Conv2d(64,128,kernel_size = 3,stride = 1),\n",
        "                                nn.BatchNorm2d(128),\n",
        "                                nn.LeakyReLU(0.5)\n",
        "                              )\n",
        "    self.conv4 = nn.Sequential(\n",
        "                                nn.Conv2d(128,256,kernel_size = 3,stride = 1),\n",
        "                                nn.BatchNorm2d(256),\n",
        "                                nn.LeakyReLU(0.5)\n",
        "                              )\n",
        "    self.fc1 = nn.Sequential( \n",
        "                                nn.Linear(1024, 512),\n",
        "                                nn.LeakyReLU(0.2),\n",
        "                                nn.Dropout(0.3)\n",
        "                            )\n",
        "    self.fc2 = nn.Sequential( \n",
        "                                nn.Linear(512, 128),\n",
        "                                nn.LeakyReLU(0.2),\n",
        "                                nn.Dropout(0.3)\n",
        "                            )\n",
        "    self.fc3 = nn.Sequential( \n",
        "                                nn.Linear(128, 1),                             \n",
        "                            )\n",
        "    \n",
        "  def forward(self,x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = x.view(-1,1024)\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.fc3(x)\n",
        "    x = F.sigmoid(x)\n",
        "    return x\n",
        "discriminator = Discriminator_1().to(device)\n",
        "summary(discriminator, (3, 32, 32))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 15, 15]           1,568\n",
            "       BatchNorm2d-2           [-1, 32, 15, 15]              64\n",
            "         LeakyReLU-3           [-1, 32, 15, 15]               0\n",
            "            Conv2d-4             [-1, 64, 6, 6]          32,832\n",
            "       BatchNorm2d-5             [-1, 64, 6, 6]             128\n",
            "         LeakyReLU-6             [-1, 64, 6, 6]               0\n",
            "            Conv2d-7            [-1, 128, 4, 4]          73,856\n",
            "       BatchNorm2d-8            [-1, 128, 4, 4]             256\n",
            "         LeakyReLU-9            [-1, 128, 4, 4]               0\n",
            "           Conv2d-10            [-1, 256, 2, 2]         295,168\n",
            "      BatchNorm2d-11            [-1, 256, 2, 2]             512\n",
            "        LeakyReLU-12            [-1, 256, 2, 2]               0\n",
            "           Linear-13                  [-1, 512]         524,800\n",
            "        LeakyReLU-14                  [-1, 512]               0\n",
            "          Dropout-15                  [-1, 512]               0\n",
            "           Linear-16                  [-1, 128]          65,664\n",
            "        LeakyReLU-17                  [-1, 128]               0\n",
            "          Dropout-18                  [-1, 128]               0\n",
            "           Linear-19                    [-1, 1]             129\n",
            "================================================================\n",
            "Total params: 994,977\n",
            "Trainable params: 994,977\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.30\n",
            "Params size (MB): 3.80\n",
            "Estimated Total Size (MB): 4.11\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "6yYV7yonwJA1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.00001)\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=0.001)\n",
        "loss = nn.BCELoss()\n",
        "loss1 = nn.MSELoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ngo0hNUozOqL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ones_target(size):\n",
        "    '''\n",
        "    Tensor containing ones, with shape = size\n",
        "    '''\n",
        "    data = Variable(torch.ones(size, 1))\n",
        "    data = data.type(torch.cuda.FloatTensor)\n",
        "    return data\n",
        "\n",
        "def zeros_target(size):\n",
        "    '''\n",
        "    Tensor containing zeros, with shape = size\n",
        "    '''\n",
        "    data = Variable(torch.zeros(size, 1))\n",
        "    data = data.type(torch.cuda.FloatTensor)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MWGEXI4SwO-N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_discriminator(real,fake,optimizer):\n",
        "  optimizer.zero_grad()\n",
        "  N = real.size(0)\n",
        "  #real_data\n",
        "  prediction_real = discriminator(real)\n",
        "  error_real = loss(prediction_real, zeros_target(N))\n",
        "  error_real.backward()\n",
        "  #fake_data\n",
        "  prediction_fake = discriminator(fake)\n",
        "  error_fake = loss(prediction_fake,ones_target(N))\n",
        "  error_fake.backward()\n",
        "  optimizer.step()\n",
        "  return (error_real + error_fake)/2, prediction_real, prediction_fake"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E2hqCyxkzXMH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_generator(optimizer, fake , real_data):\n",
        "    optimizer.zero_grad()\n",
        "    #N = fake.size(0)\n",
        "    # Sample noise and generate fake data\n",
        "    #prediction = discriminator(fake_data)\n",
        "    error = 1\n",
        "    #error = loss(prediction, zeros_target(N))\n",
        "    #error.backward(retain_graph=True)\n",
        "    #optimizer.step()\n",
        "\n",
        "    error2 = loss1(real_data,fake)\n",
        "    error2.backward()\n",
        "    optimizer.step()\n",
        "    return error,error2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BtLSzTu0J3Sl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image,output = dataset[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DPZYKAp8z9nx",
        "colab_type": "code",
        "outputId": "5b7ecfc7-adbd-452d-9934-9ce7866b27e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1318
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "\n",
        "for epoch in range(10):\n",
        "  for n_batch,(input_img,output_img) in enumerate(dataloader):\n",
        "        N = input_img.size(0)\n",
        "        # 1. Train Discriminator\n",
        "        input_img = input_img.view(N,1,32,32)\n",
        "        # Generate fake data and detach \n",
        "        # (so gradients are not calculated for generator)\n",
        "        input_img = input_img.to(device)\n",
        "        input_img = input_img.type(torch.cuda.FloatTensor)\n",
        "        output_img = output_img.view(N,3,32,32)\n",
        "        output_img = output_img.to(device)\n",
        "        output_img = output_img.type(torch.cuda.FloatTensor)\n",
        "        generator.train()\n",
        "        fake_data = generator(input_img).detach()\n",
        "        \n",
        "        # Train D\n",
        "        d_error, d_pred_real, d_pred_fake = train_discriminator(output_img, fake_data,d_optimizer)\n",
        "        # 2. Train Generator\n",
        "        # Generate fake data\n",
        "        fake_data = generator(input_img)\n",
        "        # Train G\n",
        "        g_error,reg_error = train_generator(g_optimizer, fake_data , output_img)\n",
        "        if(n_batch%300==0):\n",
        "          print('epochs : {}'.format(epoch) , 'd_error : {}'.format(d_error) , 'g_error : {}'.format(g_error) , 'reg_error : {}'.format(reg_error))\n",
        "        # Display Progress every few batches\n",
        "  torch.optim.lr_scheduler.StepLR(g_optimizer, 3, gamma=0.1, last_epoch=-1)\n",
        "  generator.eval()\n",
        "  image,_ = dataset[3]\n",
        "  image = image.reshape((32,32,1))\n",
        "  image = torch.from_numpy(image)\n",
        "  image = image.type(torch.cuda.FloatTensor)\n",
        "  image = image.view(1,1,32,32)\n",
        "  target = generator(image).detach()\n",
        "  b = target.to(torch.device(\"cpu\"))\n",
        "  b = b.numpy()\n",
        "  b = np.reshape(b,(32,32,3))\n",
        "  b = b*255\n",
        "  cv2.imwrite('/content/generated/{}.png'.format(epoch),b)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epochs : 0 d_error : 6.071189880371094 g_error : 1 reg_error : 0.49418601393699646\n",
            "epochs : 0 d_error : 0.027496974915266037 g_error : 1 reg_error : 0.11441733688116074\n",
            "epochs : 0 d_error : 0.013999528251588345 g_error : 1 reg_error : 0.050934284925460815\n",
            "epochs : 0 d_error : 0.04946921765804291 g_error : 1 reg_error : 0.04400559887290001\n",
            "epochs : 0 d_error : 0.04256166145205498 g_error : 1 reg_error : 0.03485648334026337\n",
            "epochs : 0 d_error : 0.0015248290728777647 g_error : 1 reg_error : 0.03384008631110191\n",
            "epochs : 1 d_error : 0.002281107008457184 g_error : 1 reg_error : 0.035484880208969116\n",
            "epochs : 1 d_error : 0.0032926532439887524 g_error : 1 reg_error : 0.03161979466676712\n",
            "epochs : 1 d_error : 0.02046659216284752 g_error : 1 reg_error : 0.03108309581875801\n",
            "epochs : 1 d_error : 0.055809617042541504 g_error : 1 reg_error : 0.03467024117708206\n",
            "epochs : 1 d_error : 0.0015647540567442775 g_error : 1 reg_error : 0.03090626373887062\n",
            "epochs : 1 d_error : 0.0025364644825458527 g_error : 1 reg_error : 0.03082769177854061\n",
            "epochs : 2 d_error : 0.011726008728146553 g_error : 1 reg_error : 0.036234065890312195\n",
            "epochs : 2 d_error : 0.0025233658961951733 g_error : 1 reg_error : 0.027443787083029747\n",
            "epochs : 2 d_error : 0.0030812241602689028 g_error : 1 reg_error : 0.02800881862640381\n",
            "epochs : 2 d_error : 0.0012526368955150247 g_error : 1 reg_error : 0.03232187405228615\n",
            "epochs : 2 d_error : 0.0026386051904410124 g_error : 1 reg_error : 0.028160210698843002\n",
            "epochs : 2 d_error : 0.00029232693486846983 g_error : 1 reg_error : 0.027523880824446678\n",
            "epochs : 3 d_error : 0.0022466008085757494 g_error : 1 reg_error : 0.035899799317121506\n",
            "epochs : 3 d_error : 0.00043194633326493204 g_error : 1 reg_error : 0.028506115078926086\n",
            "epochs : 3 d_error : 0.0015216449974104762 g_error : 1 reg_error : 0.028681373223662376\n",
            "epochs : 3 d_error : 0.0005284149083308876 g_error : 1 reg_error : 0.02275857888162136\n",
            "epochs : 3 d_error : 0.002288750372827053 g_error : 1 reg_error : 0.022861503064632416\n",
            "epochs : 3 d_error : 9.412779763806611e-05 g_error : 1 reg_error : 0.024662937968969345\n",
            "epochs : 4 d_error : 0.008551187813282013 g_error : 1 reg_error : 0.024589918553829193\n",
            "epochs : 4 d_error : 0.00028099268092773855 g_error : 1 reg_error : 0.027745967730879784\n",
            "epochs : 4 d_error : 0.0046973018907010555 g_error : 1 reg_error : 0.024941574782133102\n",
            "epochs : 4 d_error : 6.602418579859659e-05 g_error : 1 reg_error : 0.024190768599510193\n",
            "epochs : 4 d_error : 0.0001259101991308853 g_error : 1 reg_error : 0.02501741051673889\n",
            "epochs : 4 d_error : 7.509243732783943e-05 g_error : 1 reg_error : 0.020631764084100723\n",
            "epochs : 5 d_error : 0.022255267947912216 g_error : 1 reg_error : 0.02223094366490841\n",
            "epochs : 5 d_error : 0.0003399911511223763 g_error : 1 reg_error : 0.026293611153960228\n",
            "epochs : 5 d_error : 1.6226655134232715e-05 g_error : 1 reg_error : 0.026270341128110886\n",
            "epochs : 5 d_error : 8.092351345112547e-05 g_error : 1 reg_error : 0.02757928892970085\n",
            "epochs : 5 d_error : 6.622164801228791e-05 g_error : 1 reg_error : 0.02201170288026333\n",
            "epochs : 5 d_error : 1.370964128000196e-05 g_error : 1 reg_error : 0.025116777047514915\n",
            "epochs : 6 d_error : 0.0006166957318782806 g_error : 1 reg_error : 0.023094769567251205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-4c5126b1aaa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mfake_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Train G\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mg_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreg_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_data\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0moutput_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epochs : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'd_error : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_error\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'g_error : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_error\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'reg_error : {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-5e96bf444df4>\u001b[0m in \u001b[0;36mtrain_generator\u001b[0;34m(optimizer, fake, real_data)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0merror2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0merror2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "KlQfk679VYDf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "37206d81-bdbb-4b36-bfe8-5a4fd3bbe978"
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "c = cv2.imread('/content/generated/5.png')\n",
        "plt.imshow(c)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f730b57e470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "WqaMt-F_1OS0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir generated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q21AZNdnKy4o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class double_conv(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2'''\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(double_conv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class inconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(inconv, self).__init__()\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(down, self).__init__()\n",
        "        self.mpconv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            double_conv(in_ch, out_ch)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mpconv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
        "        super(up, self).__init__()\n",
        "\n",
        "        #  would be a nice idea if the upsampling could be learned too,\n",
        "        #  but my machine do not have enough memory to handle all those weights\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
        "\n",
        "        self.conv = double_conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        \n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
        "                        diffY // 2, diffY - diffY//2))\n",
        "        \n",
        "        # for padding issues, see \n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class outconv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(outconv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X3tV2m-hLzzz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.inc = inconv(n_channels, 64)\n",
        "        self.down1 = down(64, 128)\n",
        "        self.down2 = down(128, 256)\n",
        "        self.down3 = down(256, 512)\n",
        "        self.down4 = down(512, 512)\n",
        "        self.up1 = up(1024, 256)\n",
        "        self.up2 = up(512, 128)\n",
        "        self.up3 = up(256, 64)\n",
        "        self.up4 = up(128, 64)\n",
        "        self.outc = outconv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return F.sigmoid(x)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}